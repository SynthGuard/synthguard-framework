{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp.compiler import Compiler\n",
    "from kfp import components as comp\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies = ['git+https://mshoush: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@github.com/mshoush/synthguard.git']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IMAGE = 'gitlab.ext.cyber.ee:5050/exai/synthguard:latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "\n",
    "# from helper_functions import load_json, handle_nested_data_json, reverse_flatten\n",
    "# import pandas as pd\n",
    "\n",
    "# files_to_be_zipped = []\n",
    "\n",
    "# input_path = \"./../../data-synthesis/docs/examples/energy-pilot-teadal/datasets/\"\n",
    "# output_path = 'synthetic_datasets/teadal/'\n",
    "\n",
    "# file1 = 'sir-min-temp.json'\n",
    "# file2 = 'sir-max-temp.json'\n",
    "\n",
    "# real_data1 = handle_nested_data_json(pd.json_normalize(load_json(input_path + file1)))\n",
    "# real_data2 = handle_nested_data_json(pd.json_normalize(load_json(input_path + file2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when stripping type annotations: No module named 'lib2to3'\n"
     ]
    }
   ],
   "source": [
    "def sir_input(input_path:str,\n",
    "          output_min_csv: comp.OutputPath('csv'), \n",
    "          output_max_csv: comp.OutputPath('csv'),\n",
    "          output_json: comp.OutputPath('json')):\n",
    "    from synthguard.helper_functions import load_json, handle_nested_data_json, save_to_csv, save_json\n",
    "    import pandas as pd\n",
    "\n",
    "    file1 = 'sir-min-temp.json'\n",
    "    file2 = 'sir-max-temp.json'\n",
    "\n",
    "    real_data1 = handle_nested_data_json(pd.json_normalize(load_json(input_path + file1)))\n",
    "    real_data2 = handle_nested_data_json(pd.json_normalize(load_json(input_path + file2)))\n",
    "\n",
    "    save_to_csv(real_data1, output_min_csv)\n",
    "    save_to_csv(real_data2, output_max_csv)\n",
    "    save_json(output_json, file1, load_json(input_path + file1))\n",
    "\n",
    "input_component = comp.create_component_from_func(sir_input, base_image=BASE_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data_preprocessor import DataPreprocessor \n",
    "\n",
    "# dataPreprocessor1 = DataPreprocessor(data = real_data1)\n",
    "# processed_data1, metadata1 = dataPreprocessor1.preprocess_data()\n",
    "\n",
    "# dataPreprocessor2 = DataPreprocessor(data = real_data2)\n",
    "# processed_data2, metadata2 = dataPreprocessor2.preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when stripping type annotations: No module named 'lib2to3'\n"
     ]
    }
   ],
   "source": [
    "def sir_preprocess(input_min_csv: comp.InputPath('csv'), \n",
    "               input_max_csv: comp.InputPath('csv'),\n",
    "               output_min_metadata: comp.OutputPath('json'),\n",
    "               output_max_metadata: comp.OutputPath('json')):\n",
    "    from synthguard.data_preprocessor import DataPreprocessor\n",
    "    from synthguard.helper_functions import save_metadata, load_data_csv\n",
    "\n",
    "    real_data1 = load_data_csv(input_min_csv)\n",
    "    real_data2 = load_data_csv(input_max_csv)\n",
    "\n",
    "    dataPreprocessor1 = DataPreprocessor(data = real_data1)\n",
    "    processed_data1, metadata1 = dataPreprocessor1.preprocess_data()\n",
    "    save_metadata(metadata1, output_min_metadata)\n",
    "\n",
    "    dataPreprocessor2 = DataPreprocessor(data = real_data2)\n",
    "    processed_data2, metadata2 = dataPreprocessor2.preprocess_data()\n",
    "    save_metadata(metadata2, output_max_metadata)\n",
    "\n",
    "preprocess_comp = comp.create_component_from_func(sir_preprocess, base_image=BASE_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from synthetic_data_generator import SyntheticDataGenerator\n",
    "\n",
    "\n",
    "# output1_json = 'sir-min-temp-synthetic.csv'\n",
    "# output2_json = 'sir-max-temp-synthetic.csv'\n",
    "\n",
    "\n",
    "# if output1_json:\n",
    "#     # Create the output path if it does not exist\n",
    "#     import os\n",
    "#     if not os.path.exists(output_path):\n",
    "#         os.makedirs(output_path)\n",
    "#     output1 = output_path + output1_json\n",
    "\n",
    "\n",
    "# if output2_json:\n",
    "#     # Create the output path if it does not exist\n",
    "#     import os\n",
    "#     if not os.path.exists(output_path):\n",
    "#         os.makedirs(output_path)\n",
    "#     output2 = output_path + output1_json\n",
    "\n",
    "\n",
    "# N_Rows = 1000\n",
    "# EPOCHS = 1\n",
    "# Locales = 'ee_ET'\n",
    "# synthetic_data_type = 'realistic'\n",
    "\n",
    "# syntheticDataGenerator1 = SyntheticDataGenerator(locales=Locales, n_rows=N_Rows, method=synthetic_data_type, output_csv=output1)\n",
    "# generated_data1 = syntheticDataGenerator1.generate_synthetic_data(metadata = metadata1, processed_data = processed_data1, Nepochs=EPOCHS)\n",
    "\n",
    "# syntheticDataGenerator2 = SyntheticDataGenerator(locales=Locales, n_rows=N_Rows, method=synthetic_data_type, output_csv=output2)\n",
    "# generated_data2 = syntheticDataGenerator2.generate_synthetic_data(metadata = metadata2, processed_data = processed_data2, Nepochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when stripping type annotations: No module named 'lib2to3'\n"
     ]
    }
   ],
   "source": [
    "def sir_generation(n_rows: int,\n",
    "               input_min_csv: comp.InputPath('csv'),\n",
    "               input_max_csv: comp.InputPath('csv'),\n",
    "               input_min_metadata: comp.InputPath('json'),\n",
    "               input_max_metadata: comp.InputPath('json'),\n",
    "               output_min_csv:comp.OutputPath('csv'),\n",
    "               output_max_csv:comp.OutputPath('csv')):\n",
    "    from synthguard.synthetic_data_generator import SyntheticDataGenerator\n",
    "    from synthguard.helper_functions import load_metadata, load_data_csv, save_to_csv\n",
    "\n",
    "    metadata1 = load_metadata(input_min_metadata)\n",
    "    metadata2 = load_metadata(input_max_metadata)\n",
    "\n",
    "    processed_data1 =  load_data_csv(input_min_csv)\n",
    "    processed_data2 = load_data_csv(input_max_csv)\n",
    "\n",
    "    N_Rows = n_rows\n",
    "    EPOCHS = 1\n",
    "    Locales = 'ee_ET'\n",
    "    synthetic_data_type = 'realistic'\n",
    "\n",
    "    syntheticDataGenerator1 = SyntheticDataGenerator(locales=Locales, n_rows=N_Rows, method=synthetic_data_type, output_csv=output_min_csv)\n",
    "    generated_data1 = syntheticDataGenerator1.generate_synthetic_data(metadata = metadata1, processed_data = processed_data1, Nepochs=EPOCHS)\n",
    "\n",
    "    syntheticDataGenerator2 = SyntheticDataGenerator(locales=Locales, n_rows=N_Rows, method=synthetic_data_type, output_csv=output_max_csv)\n",
    "    generated_data2 = syntheticDataGenerator2.generate_synthetic_data(metadata = metadata2, processed_data = processed_data2, Nepochs=EPOCHS)\n",
    "\n",
    "generation_comp = comp.create_component_from_func(sir_generation, base_image=BASE_IMAGE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when stripping type annotations: No module named 'lib2to3'\n"
     ]
    }
   ],
   "source": [
    "def sir_combine(input_min_csv: comp.InputPath('csv'),\n",
    "                input_max_csv: comp.InputPath('csv'),\n",
    "                input_json: comp.InputPath('json'),\n",
    "                output_combined_json: comp.OutputPath('json')):\n",
    "    import os\n",
    "    from synthguard.helper_functions import reverse_flatten, save_json, load_data_csv, load_json\n",
    "\n",
    "    # Debug input and output paths\n",
    "    input_json_path = os.path.join(input_json, 'sir-min-temp.json')\n",
    "    output_combined = 'sir-min-max-temp-synthetic.json'\n",
    "    \n",
    "    print(f\"input_min_csv path: {input_min_csv}\")\n",
    "    print(f\"input_max_csv path: {input_max_csv}\")\n",
    "    print(f\"input_json path: {input_json_path}\")\n",
    "    print(f\"output_combined_json path: {output_combined_json}\")\n",
    "    \n",
    "    # Load CSV data and original JSON structure\n",
    "    generated_data1 = load_data_csv(input_min_csv)\n",
    "    generated_data2 = load_data_csv(input_max_csv)\n",
    "    original_json_structure = load_json(input_json_path)\n",
    "\n",
    "    # Validate that the JSON structure is a dictionary\n",
    "    if not isinstance(original_json_structure, dict):\n",
    "        raise ValueError(f\"Expected a dictionary from {input_json_path}, but got {type(original_json_structure)}\")\n",
    "\n",
    "    # Reverse-flatten the generated data using the original JSON structure\n",
    "    rebuilt_data1 = reverse_flatten(generated_data1, original_json_structure)\n",
    "    rebuilt_data2 = reverse_flatten(generated_data2, original_json_structure)\n",
    "\n",
    "    # Combine the data\n",
    "    combined_data = {0: rebuilt_data1, 1: rebuilt_data2}\n",
    "\n",
    "    # Ensure output directory exists and save the combined JSON\n",
    "    os.makedirs(os.path.dirname(output_combined_json), exist_ok=True)\n",
    "    save_json(output_combined_json, output_combined, combined_data)\n",
    "\n",
    "    print(f\"Merged JSON saved to {os.path.join(output_combined_json, output_combined)}\")\n",
    "    \n",
    "# Define the component\n",
    "combine_comp = comp.create_component_from_func(sir_combine, base_image=BASE_IMAGE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnostic Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from diagnostic_report_generator import DiagnosticEvaluator\n",
    "\n",
    "# diagnosticReportGenerator1 = DiagnosticEvaluator(real_data = processed_data1, synthetic_data = generated_data1, metadata = metadata1, method=synthetic_data_type)\n",
    "# diagnosticReportGenerator1.run_diagnostic_realistic()\n",
    "# diagnosticReportGenerator1.plot_diagnostic_report_realistic(output_path = output_path)\n",
    "\n",
    "# diagnosticReportGenerator2 = DiagnosticEvaluator(real_data = processed_data2, synthetic_data = generated_data2, metadata = metadata2, method=synthetic_data_type)\n",
    "# diagnosticReportGenerator2.run_diagnostic_realistic()\n",
    "# diagnosticReportGenerator2.plot_diagnostic_report_realistic(output_path = output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when stripping type annotations: No module named 'lib2to3'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def sir_diagnostic(input_min_real_csv: comp.InputPath('csv'),\n",
    "               input_max_real_csv: comp.InputPath('csv'),\n",
    "               input_min_synth_csv: comp.InputPath('csv'),\n",
    "               input_max_synth_csv: comp.InputPath('csv'),\n",
    "               input_min_metadata: comp.InputPath('json'),\n",
    "               input_max_metadata: comp.InputPath('json'),\n",
    "               output_html: comp.OutputPath('html')) -> NamedTuple('VisualizationOutput', [('mlpipeline_ui_metadata', 'UI_metadata')]):\n",
    "    from synthguard.diagnostic_report_generator import DiagnosticEvaluator\n",
    "    from synthguard.helper_functions import load_data_csv, load_metadata\n",
    "    import json\n",
    "\n",
    "    with open(output_html, \"w\") as f:\n",
    "        f.write(\"<html><body>\")  # Start the HTML document\n",
    "\n",
    "    processed_data1 =  load_data_csv(input_min_real_csv)\n",
    "    processed_data2 = load_data_csv(input_max_real_csv)\n",
    "\n",
    "    generated_data1 = load_data_csv(input_min_synth_csv)\n",
    "    generated_data2 = load_data_csv(input_max_synth_csv)\n",
    "\n",
    "    metadata1 = load_metadata(input_min_metadata)\n",
    "    metadata2 = load_metadata(input_max_metadata)\n",
    "\n",
    "    synthetic_data_type = 'realistic'\n",
    "\n",
    "    diagnosticReportGenerator1 = DiagnosticEvaluator(real_data = processed_data1, synthetic_data = generated_data1, metadata = metadata1, method=synthetic_data_type)\n",
    "    diagnosticReportGenerator1.run_diagnostic_realistic()\n",
    "    diagnosticReportGenerator1.plot_diagnostic_report_realistic()\n",
    "    diagnosticReportGenerator1.save_plot_to_html(output_html)\n",
    "\n",
    "    diagnosticReportGenerator2 = DiagnosticEvaluator(real_data = processed_data2, synthetic_data = generated_data2, metadata = metadata2, method=synthetic_data_type)\n",
    "    diagnosticReportGenerator2.run_diagnostic_realistic()\n",
    "    diagnosticReportGenerator2.plot_diagnostic_report_realistic()\n",
    "    diagnosticReportGenerator2.save_plot_to_html(output_html)\n",
    "\n",
    "    # Read the HTML content for UI metadata\n",
    "    with open(output_html, 'r') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    metadata = {\n",
    "        'outputs': [{\n",
    "            'type': 'web-app',\n",
    "            'storage': 'inline',\n",
    "            'source': html_content,\n",
    "        }]\n",
    "    }\n",
    "\n",
    "    from collections import namedtuple\n",
    "    visualization_output = namedtuple('VisualizationOutput', ['mlpipeline_ui_metadata'])\n",
    "    return visualization_output(json.dumps(metadata))\n",
    "\n",
    "diagnostic_component = comp.create_component_from_func(sir_diagnostic, base_image=BASE_IMAGE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from synthguard.quality_report_generator import DataQualityEvaluator\n",
    "\n",
    "# dataQualityEvaluator1 = DataQualityEvaluator(real_data = processed_data1, synthetic_data = generated_data1, metadata = metadata1, method=synthetic_data_type)\n",
    "# dataQualityEvaluator1.evaluate_quality()\n",
    "# dataQualityEvaluator1.plot_quality_report_realistic(output_path = output_path)\n",
    "\n",
    "\n",
    "# dataQualityEvaluator2 = DataQualityEvaluator(real_data = processed_data2, synthetic_data = generated_data2, metadata = metadata2, method=synthetic_data_type)\n",
    "# dataQualityEvaluator2.evaluate_quality()\n",
    "# dataQualityEvaluator2.plot_quality_report_realistic(output_path = output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when stripping type annotations: No module named 'lib2to3'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def sir_quality(input_min_real_csv: comp.InputPath('csv'),\n",
    "               input_max_real_csv: comp.InputPath('csv'),\n",
    "               input_min_synth_csv: comp.InputPath('csv'),\n",
    "               input_max_synth_csv: comp.InputPath('csv'),\n",
    "               input_min_metadata: comp.InputPath('json'),\n",
    "               input_max_metadata: comp.InputPath('json'),\n",
    "               output_html: comp.OutputPath('html')) -> NamedTuple('VisualizationOutput', [('mlpipeline_ui_metadata', 'UI_metadata')]):\n",
    "    from synthguard.quality_report_generator import DataQualityEvaluator\n",
    "    from synthguard.helper_functions import load_data_csv, load_metadata\n",
    "    import json\n",
    "\n",
    "    with open(output_html, \"w\") as f:\n",
    "        f.write(\"<html><body>\")  # Start the HTML document\n",
    "\n",
    "    processed_data1 =  load_data_csv(input_min_real_csv)\n",
    "    processed_data2 = load_data_csv(input_max_real_csv)\n",
    "\n",
    "    generated_data1 = load_data_csv(input_min_synth_csv)\n",
    "    generated_data2 = load_data_csv(input_max_synth_csv)\n",
    "\n",
    "    metadata1 = load_metadata(input_min_metadata)\n",
    "    metadata2 = load_metadata(input_max_metadata)\n",
    "\n",
    "    synthetic_data_type = 'realistic'\n",
    "\n",
    "    dataQualityEvaluator1 = DataQualityEvaluator(real_data = processed_data1, synthetic_data = generated_data1, metadata = metadata1, method=synthetic_data_type)\n",
    "    dataQualityEvaluator1.evaluate_quality()\n",
    "    dataQualityEvaluator1.plot_quality_report_realistic()\n",
    "    dataQualityEvaluator1.save_plot_to_html(output_html)\n",
    "\n",
    "    dataQualityEvaluator2 = DataQualityEvaluator(real_data = processed_data2, synthetic_data = generated_data2, metadata = metadata2, method=synthetic_data_type)\n",
    "    dataQualityEvaluator2.evaluate_quality()\n",
    "    dataQualityEvaluator2.plot_quality_report_realistic()\n",
    "    dataQualityEvaluator2.save_plot_to_html(output_html)\n",
    "\n",
    "    # Read the HTML content for UI metadata\n",
    "    with open(output_html, 'r') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    metadata = {\n",
    "        'outputs': [{\n",
    "            'type': 'web-app',\n",
    "            'storage': 'inline',\n",
    "            'source': html_content,\n",
    "        }]\n",
    "    }\n",
    "\n",
    "    from collections import namedtuple\n",
    "    visualization_output = namedtuple('VisualizationOutput', ['mlpipeline_ui_metadata'])\n",
    "    return visualization_output(json.dumps(metadata))\n",
    "\n",
    "quality_comp = comp.create_component_from_func(sir_quality, base_image=BASE_IMAGE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Privacy Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from privacy_report_generator import PrivacyRiskEvaluator\n",
    "\n",
    "# privacyRiskEvaluator1 = PrivacyRiskEvaluator(real_data = processed_data1, synthetic_data = generated_data1, metadata = metadata1, method=synthetic_data_type)\n",
    "# privacyRiskEvaluator1.run_privacy_realistic()\n",
    "# privacyRiskEvaluator1.plot_privacy_metrics_realistic(output_path = output_path)\n",
    "\n",
    "# privacyRiskEvaluator2 = PrivacyRiskEvaluator(real_data = processed_data2, synthetic_data = generated_data2, metadata = metadata2, method=synthetic_data_type)       \n",
    "# privacyRiskEvaluator2.run_privacy_realistic()\n",
    "# privacyRiskEvaluator2.plot_privacy_metrics_realistic(output_path = output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when stripping type annotations: No module named 'lib2to3'\n"
     ]
    }
   ],
   "source": [
    "def sir_privacy(input_min_real_csv: comp.InputPath('csv'),\n",
    "               input_max_real_csv: comp.InputPath('csv'),\n",
    "               input_min_synth_csv: comp.InputPath('csv'),\n",
    "               input_max_synth_csv: comp.InputPath('csv'),\n",
    "               input_min_metadata: comp.InputPath('json'),\n",
    "               input_max_metadata: comp.InputPath('json'),\n",
    "               output_html: comp.OutputPath('html')) -> NamedTuple('VisualizationOutput', [('mlpipeline_ui_metadata', 'UI_metadata')]):\n",
    "    from synthguard.privacy_report_generator import PrivacyRiskEvaluator\n",
    "    from synthguard.helper_functions import load_data_csv, load_metadata\n",
    "    import json\n",
    "\n",
    "    with open(output_html, \"w\") as f:\n",
    "        f.write(\"<html><body>\")  # Start the HTML document\n",
    "\n",
    "    processed_data1 =  load_data_csv(input_min_real_csv)\n",
    "    processed_data2 = load_data_csv(input_max_real_csv)\n",
    "\n",
    "    generated_data1 = load_data_csv(input_min_synth_csv)\n",
    "    generated_data2 = load_data_csv(input_max_synth_csv)\n",
    "\n",
    "    metadata1 = load_metadata(input_min_metadata)\n",
    "    metadata2 = load_metadata(input_max_metadata)\n",
    "\n",
    "    synthetic_data_type = 'realistic'    \n",
    "\n",
    "    privacyRiskEvaluator1 = PrivacyRiskEvaluator(real_data = processed_data1, synthetic_data = generated_data1, metadata = metadata1, method=synthetic_data_type)\n",
    "    privacyRiskEvaluator1.run_privacy_realistic()\n",
    "    privacyRiskEvaluator1.plot_privacy_metrics_realistic()\n",
    "    privacyRiskEvaluator1.save_plot_to_html(output_html)\n",
    "\n",
    "    privacyRiskEvaluator2 = PrivacyRiskEvaluator(real_data = processed_data2, synthetic_data = generated_data2, metadata = metadata2, method=synthetic_data_type)       \n",
    "    privacyRiskEvaluator2.run_privacy_realistic()\n",
    "    privacyRiskEvaluator2.plot_privacy_metrics_realistic()\n",
    "    privacyRiskEvaluator2.save_plot_to_html(output_html)\n",
    "        # Read the HTML content for UI metadata\n",
    "    with open(output_html, 'r') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    metadata = {\n",
    "        'outputs': [{\n",
    "            'type': 'web-app',\n",
    "            'storage': 'inline',\n",
    "            'source': html_content,\n",
    "        }]\n",
    "    }\n",
    "\n",
    "    from collections import namedtuple\n",
    "    visualization_output = namedtuple('VisualizationOutput', ['mlpipeline_ui_metadata'])\n",
    "    return visualization_output(json.dumps(metadata))\n",
    "\n",
    "privacy_comp = comp.create_component_from_func(sir_privacy, base_image=BASE_IMAGE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEADAL half pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsl.pipeline(name='TEADAL_half_pipeline')\n",
    "def pipeline(n_rows:int):\n",
    "    #PVC init\n",
    "    existing_pvc = dsl.PipelineVolume(pvc='my-pvc')\n",
    "\n",
    "    input_sir = input_component('/mnt/pvc/').add_pvolumes({\"/mnt/pvc\": existing_pvc})\n",
    "\n",
    "    preprocess_sir = preprocess_comp(input_sir.outputs['output_min_csv'], input_sir.outputs['output_max_csv'])\n",
    "\n",
    "    generation_sir = generation_comp(n_rows,\n",
    "                                 input_sir.outputs['output_min_csv'], \n",
    "                                 input_sir.outputs['output_max_csv'],\n",
    "                                 preprocess_sir.outputs['output_min_metadata'],\n",
    "                                 preprocess_sir.outputs['output_max_metadata'])\n",
    "    \n",
    "    diagnostic_sir = diagnostic_component(input_sir.outputs['output_min_csv'], \n",
    "                                 input_sir.outputs['output_max_csv'],\n",
    "                                 generation_sir.outputs['output_min_csv'],\n",
    "                                 generation_sir.outputs['output_max_csv'],\n",
    "                                 preprocess_sir.outputs['output_min_metadata'],\n",
    "                                 preprocess_sir.outputs['output_max_metadata'])\n",
    "    \n",
    "    utility_sir = quality_comp(input_sir.outputs['output_min_csv'], \n",
    "                                 input_sir.outputs['output_max_csv'],\n",
    "                                 generation_sir.outputs['output_min_csv'],\n",
    "                                 generation_sir.outputs['output_max_csv'],\n",
    "                                 preprocess_sir.outputs['output_min_metadata'],\n",
    "                                 preprocess_sir.outputs['output_max_metadata'])\n",
    "    \n",
    "    privacy_sir = privacy_comp(input_sir.outputs['output_min_csv'], \n",
    "                                 input_sir.outputs['output_max_csv'],\n",
    "                                 generation_sir.outputs['output_min_csv'],\n",
    "                                 generation_sir.outputs['output_max_csv'],\n",
    "                                 preprocess_sir.outputs['output_min_metadata'],\n",
    "                                 preprocess_sir.outputs['output_max_metadata'])\n",
    "    \n",
    "Compiler().compile(pipeline, 'teadal_half_pipeline.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text files street_names and municipalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from faker import Faker\n",
    "# import pandas as pd\n",
    "# from generate_personal_data import PersonalFaker\n",
    "# from helper_functions import read_txt_and_convert_to_df\n",
    "\n",
    "\n",
    "\n",
    "# # Create an instance of PersonalFaker\n",
    "# estonian_fake = PersonalFaker(\"it_IT\")\n",
    "\n",
    "# # input_path = \"./../../data-synthesis/docs/examples/energy-pilot-teadal/datasets/\"\n",
    "\n",
    "# input_file1 = \"street_names.txt\"\n",
    "# input_file2 = \"municipality_codes.txt\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Read and truncate files to the same length\n",
    "# street_names, municipality_codes = (read_txt_and_convert_to_df(input_path, f) for f in [input_file1, input_file2])\n",
    "# min_rows = min(len(street_names), len(municipality_codes))\n",
    "\n",
    "# # Create the DataFrame with 'id' and 'address'\n",
    "# real_data_addresses = pd.DataFrame({\n",
    "#     'id': range(1, min_rows + 1),\n",
    "#     'address': street_names.iloc[:min_rows].squeeze() + \", \" + municipality_codes.iloc[:min_rows].squeeze().astype(str)\n",
    "# })\n",
    "\n",
    "# real_data_addresses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when stripping type annotations: No module named 'lib2to3'\n"
     ]
    }
   ],
   "source": [
    "def load_streets_and_municipalities(input_path:str)->dict:\n",
    "    from synthguard.helper_functions import read_txt_and_convert_to_df\n",
    "\n",
    "    input_file1 = input_path+\"street_names.txt\"\n",
    "    input_file2 = input_path+\"municipality_codes.txt\"\n",
    "\n",
    "    street_names, municipality_codes = (read_txt_and_convert_to_df(input_path, f) for f in [input_file1, input_file2])\n",
    "\n",
    "    print(street_names.head())\n",
    "    print(municipality_codes.head())\n",
    "    \n",
    "    streets_and_municipalities = {\n",
    "        'streets': street_names.values.tolist(),\n",
    "        'municipalities': municipality_codes.values.tolist()\n",
    "    }\n",
    "    return streets_and_municipalities\n",
    "\n",
    "load_streets_and_municipalities_comp = comp.create_component_from_func(load_streets_and_municipalities, base_image=BASE_IMAGE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data_preprocessor import DataPreprocessor \n",
    "\n",
    "# dataPreprocessor = DataPreprocessor(data = real_data_addresses)\n",
    "# processed_data_addresses, metadata_addresses = dataPreprocessor.preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from synthguard.generate_personal_data import PersonalFaker\n",
    "# italian_fake = PersonalFaker(\"it_IT\")\n",
    "\n",
    "# # Generate 10 addresses\n",
    "# synthetic_addresses = italian_fake.generate_data_addresses(street_names, municipality_codes, n_addresses=real_data_addresses.shape[0])\n",
    "\n",
    "# # Print the resulting DataFrame\n",
    "# print(synthetic_addresses.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when stripping type annotations: No module named 'lib2to3'\n"
     ]
    }
   ],
   "source": [
    "def generate_addresses(n_rows:int, input_dict: dict, output_csv: comp.OutputPath('csv')):\n",
    "    from synthguard.generate_personal_data import PersonalFaker\n",
    "    from synthguard.helper_functions import save_to_csv\n",
    "    import pandas as pd \n",
    "\n",
    "    italian_fake = PersonalFaker(\"it_IT\")\n",
    "    streets = input_dict['streets']\n",
    "    municipality_codes = [code[0] for code in input_dict['municipalities']]\n",
    "    synthetic_addresses = italian_fake.generate_data_addresses(pd.Series(streets), pd.Series(municipality_codes), n_addresses=n_rows)\n",
    "\n",
    "    print(synthetic_addresses.head())\n",
    "\n",
    "    save_to_csv(synthetic_addresses, output_csv)\n",
    "\n",
    "address_generation_component = comp.create_component_from_func(generate_addresses, base_image=BASE_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# from diagnostic_report_generator import DiagnosticEvaluator\n",
    "\n",
    "# OutputCSV = 'addresses_synthetic.csv'\n",
    "# # output_path = 'synthetic_datasets/teadal/'\n",
    "\n",
    "\n",
    "# if OutputCSV:\n",
    "#     # Create the output path if it does not exist\n",
    "#     import os\n",
    "#     if not os.path.exists(output_path):\n",
    "#         os.makedirs(output_path)\n",
    "#     OutputCSV = output_path + OutputCSV\n",
    "    \n",
    "\n",
    "\n",
    "# diagnosticReportGenerator = DiagnosticEvaluator(real_data = processed_data_addresses, synthetic_data = synthetic_addresses, metadata = metadata_addresses)\n",
    "# diagnosticReportGenerator.run_diagnostic_realistic()\n",
    "# diagnosticReportGenerator.plot_diagnostic_report_realistic(output_path = output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from quality_report_generator import DataQualityEvaluator\n",
    "\n",
    "# dataQualityEvaluator = DataQualityEvaluator(real_data = processed_data_addresses, synthetic_data = synthetic_addresses, metadata = metadata_addresses, method=synthetic_data_type)\n",
    "# dataQualityEvaluator.evaluate_quality()\n",
    "# dataQualityEvaluator.plot_quality_report_realistic(output_path = output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from privacy_report_generator import PrivacyRiskEvaluator\n",
    "\n",
    "# privacyRiskEvaluator = PrivacyRiskEvaluator(real_data = processed_data_addresses, synthetic_data = synthetic_addresses, metadata = metadata_addresses, method=synthetic_data_type)\n",
    "# privacyRiskEvaluator.run_privacy_realistic()\n",
    "# privacyRiskEvaluator.plot_privacy_metrics_realistic(output_path = output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RT-CIT\n",
    "\n",
    "RT-CIT\n",
    "RT CIT thermal group:\n",
    "\n",
    "dimension [cadastre_code] - 10 characters regional code for plant identificationdimension [thermal_unit] - 4 characters thermal group code, ex: GT01\n",
    "\n",
    "\n",
    "dimension [plant_address] - street, number, building, staircase, ...dimension [municipality] - 6 characters istat municipality code, ex: 50001measure [combustion_efficiency] - double precision value between 0 and 1\n",
    "Volume:\n",
    "There are currently 1.851.142 registered plants, 5.595.063 energy efficiency check reports (RCEE) and 3.636 accredited maintenance technicians able to access and update the archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import string\n",
    "# from generate_personal_data import PersonalFaker\n",
    "\n",
    "# # output_path = 'synthetic_datasets/teadal/'\n",
    "\n",
    "# # Create an instance of PersonalFaker\n",
    "# estonian_fake = PersonalFaker(\"et_EE\")\n",
    "\n",
    "# # Lambda functions to generate cadastre and thermal unit codes\n",
    "# generate_cadastre_code = lambda: estonian_fake.generate_code(10)\n",
    "# generate_thermal_unit_code = lambda: \"GT\" + estonian_fake.generate_code(2, string.digits)\n",
    "\n",
    "# # Parameters\n",
    "# n_addresses = synthetic_addresses.shape[0]\n",
    "# n_reports_per_address = 3\n",
    "\n",
    "\n",
    "# cadastre_codes_thermal_units = estonian_fake.generate_cadastre_and_thermal_units(n_addresses, n_reports_per_address)\n",
    "\n",
    "# # generate the rt-cit-thermal-group data\n",
    "# rt_cit_thermal_group = estonian_fake.generate_rt_cit_thermal_group(synthetic_addresses, cadastre_codes_thermal_units, n_reports_per_address)\n",
    "# rt_cit_thermal_group.to_csv(output_path + \"rt-cit-thermal-group.csv\", index=False)\n",
    "\n",
    "\n",
    "# files_to_be_zipped.append(output_path + \"rt-cit-thermal-group.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when stripping type annotations: No module named 'lib2to3'\n"
     ]
    }
   ],
   "source": [
    "def rtcit(input_csv: comp.InputPath('csv'), output_csv: comp.OutputPath('csv'), n_reports_per_address:int=3):\n",
    "    from synthguard.generate_personal_data import PersonalFaker\n",
    "    from synthguard.helper_functions import load_data_csv\n",
    "\n",
    "    # Create an instance of PersonalFaker\n",
    "    italian_fake = PersonalFaker(\"it_IT\")\n",
    "\n",
    "    synthetic_addresses = load_data_csv(input_csv)\n",
    "\n",
    "    # Parameters\n",
    "    n_addresses = synthetic_addresses.shape[0]\n",
    "\n",
    "    cadastre_codes_thermal_units = italian_fake.generate_cadastre_and_thermal_units(n_addresses, n_reports_per_address)\n",
    "\n",
    "    # generate the rt-cit-thermal-group data\n",
    "    rt_cit_thermal_group = italian_fake.generate_rt_cit_thermal_group(synthetic_addresses, cadastre_codes_thermal_units, n_reports_per_address)\n",
    "    rt_cit_thermal_group.to_csv(output_csv, index=False)\n",
    "\n",
    "rtcit_component = comp.create_component_from_func(rtcit, base_image=BASE_IMAGE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  RT-APE\n",
    "\n",
    "* RT-APE dataset:\n",
    "    * dimension [address]\n",
    "    * dimension [municipality]\n",
    "    * measure [energy_rating]: A4, A3, A2, A1, B, C, D, E, F, G\n",
    "\n",
    "* Volume: \n",
    "    * APE currently contains 420.102 energy performance certificates, 443.142 registered units and 11.214 accredited certifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from generate_personal_data import PersonalFaker\n",
    "# import os\n",
    "\n",
    "# # packages_to_install = [\"xmlschema\"]\n",
    "\n",
    "# # Create an instance of PersonalFaker\n",
    "# RT_APE_fake = PersonalFaker()\n",
    "\n",
    "# # Generate synthetic addresses and municipality codes\n",
    "# addresses = rt_cit_thermal_group['address'].tolist()\n",
    "# municipality_codes = rt_cit_thermal_group['municipality_code'].tolist()\n",
    "\n",
    "# # Define energy ratings and number of certificates\n",
    "# energy_ratings = [\"A4\", \"A3\", \"A2\", \"A1\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"]\n",
    "# num_certificates = 200\n",
    "\n",
    "# # Define file paths for the schema and example XML template\n",
    "# xsd_file_path = os.path.join(input_path, 'rt-ape-schema.xsd')\n",
    "\n",
    "# # Define the path to the example XML file\n",
    "# xml_file_path = os.path.join(input_path, 'rt-ape-example.xml')\n",
    "\n",
    "\n",
    "\n",
    "# # Generate XML files based on the template and schema and return the zipped files path\n",
    "# RT_APE_files = RT_APE_fake.generate_xml_files_from_template(input_path, output_path, xsd_file_path, xml_file_path,\n",
    "#                                          addresses, municipality_codes, num_certificates, energy_ratings,)\n",
    "\n",
    "# # Add the zip file path to the list\n",
    "# files_to_be_zipped.append(RT_APE_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when stripping type annotations: No module named 'lib2to3'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def rtape(input_path:str, input_csv: comp.InputPath('csv'), output_zip: comp.OutputPath('zip'), num_certificates:int = 200):\n",
    "    from synthguard.generate_personal_data import PersonalFaker\n",
    "    import os\n",
    "    from synthguard.helper_functions import load_data_csv\n",
    "\n",
    "    # packages_to_install = [\"xmlschema\"]\n",
    "\n",
    "    # Create an instance of PersonalFaker\n",
    "    RT_APE_fake = PersonalFaker()\n",
    "\n",
    "    rt_cit_thermal_group = load_data_csv(input_csv)\n",
    "\n",
    "    # Generate synthetic addresses and municipality codes\n",
    "    addresses = rt_cit_thermal_group['address'].tolist()\n",
    "    municipality_codes = rt_cit_thermal_group['municipality_code'].tolist()\n",
    "\n",
    "    # Define energy ratings and number of certificates\n",
    "    energy_ratings = [\"A4\", \"A3\", \"A2\", \"A1\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"]\n",
    "\n",
    "    # Define file paths for the schema and example XML template\n",
    "    xsd_file_path = os.path.join(input_path, 'rt-ape-schema.xsd')\n",
    "\n",
    "    # Define the path to the example XML file\n",
    "    xml_file_path = os.path.join(input_path, 'rt-ape-example.xml')\n",
    "\n",
    "\n",
    "\n",
    "    # Generate XML files based on the template and schema and return the zipped files path\n",
    "    RT_APE_files = RT_APE_fake.generate_xml_files_from_template(input_path, output_zip, xsd_file_path, xml_file_path,\n",
    "                                            addresses, municipality_codes, num_certificates, energy_ratings,)\n",
    "\n",
    "rtape_component = comp.create_component_from_func(rtape, base_image=BASE_IMAGE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARPAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when stripping type annotations: No module named 'lib2to3'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_arpat(input_path:str, output_pm10_csv: comp.OutputPath('csv'), output_pm25_csv: comp.OutputPath('csv')):\n",
    "    from synthguard.helper_functions import load_json, handle_nested_data_json\n",
    "    import pandas as pd\n",
    "\n",
    "    arpat_file = 'arpat.json'\n",
    "\n",
    "    real_arpat = handle_nested_data_json(pd.json_normalize(load_json(input_path +'/'+ arpat_file)))\n",
    "\n",
    "    pm10_column = 'PM10'\n",
    "    pm2dot5_column = 'PM2dot5'\n",
    "\n",
    "    real_arpat_PM10 = real_arpat.drop(columns=[pm2dot5_column])\n",
    "    real_arpat_PM25 = real_arpat.drop(columns=[pm10_column])\n",
    "\n",
    "    real_arpat_PM10.to_csv(output_pm10_csv)\n",
    "    real_arpat_PM25.to_csv(output_pm25_csv)\n",
    "\n",
    "load_arpat_component = comp.create_component_from_func(load_arpat, base_image=BASE_IMAGE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when stripping type annotations: No module named 'lib2to3'\n"
     ]
    }
   ],
   "source": [
    "def arpat_preprocessor(input_pm10_csv:comp.InputPath('csv'), \n",
    "                       input_pm25_csv:comp.InputPath('csv'),\n",
    "                       output_pm10_csv: comp.OutputPath('csv'),\n",
    "                       output_pm25_csv: comp.OutputPath('csv'),\n",
    "                       output_pm10_metadata: comp.OutputPath('json'),\n",
    "                       output_pm25_metadata: comp.OutputPath('json')):\n",
    "    from synthguard.data_preprocessor import DataPreprocessor\n",
    "    from synthguard.helper_functions import load_data_csv, save_metadata, save_to_csv\n",
    "\n",
    "    real_arpat_PM10 = load_data_csv(input_pm10_csv)\n",
    "    real_arpat_PM25 = load_data_csv(input_pm25_csv)\n",
    "\n",
    "    pm10_column = 'PM10'\n",
    "    pm2dot5_column = 'PM2dot5'\n",
    "\n",
    "    columns_dict_pm10 = {\n",
    "        \"DATA_OSSERVAZIONE\": \"datetime64[ns]\",\n",
    "        'PM10': 'float64',\n",
    "        'COMUNE': 'string',\n",
    "    }\n",
    "\n",
    "    columns_dict_pm25 = {\n",
    "        \"DATA_OSSERVAZIONE\": \"datetime64[ns]\",\n",
    "        'PM2dot5': 'float64',\n",
    "        'COMUNE': 'string',\n",
    "    }\n",
    "\n",
    "    columns_to_drop = [pm10_column, pm2dot5_column]\n",
    "\n",
    "\n",
    "    dataPreprocessor_arpat_PM10 = DataPreprocessor(data = real_arpat_PM10,)\n",
    "    processed_data_arpat_PM10, metadata_arpat_PM10 = dataPreprocessor_arpat_PM10.preprocess_data(columns_dict=columns_dict_pm10, columns_to_drop=columns_to_drop)\n",
    "\n",
    "    save_metadata(metadata_arpat_PM10, output_pm10_metadata)\n",
    "    save_to_csv(processed_data_arpat_PM10, output_pm10_csv)\n",
    "\n",
    "    dataPreprocessor_arpat_PM25 = DataPreprocessor(data = real_arpat_PM25,)\n",
    "    processed_data_arpat_PM25, metadata_arpat_PM25 = dataPreprocessor_arpat_PM25.preprocess_data(columns_dict=columns_dict_pm25, columns_to_drop=columns_to_drop)\n",
    "\n",
    "    save_metadata(metadata_arpat_PM25, output_pm25_metadata)\n",
    "    save_to_csv(processed_data_arpat_PM25, output_pm25_csv)\n",
    "\n",
    "arpat_preprocessor_component = comp.create_component_from_func(arpat_preprocessor, base_image=BASE_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when stripping type annotations: No module named 'lib2to3'\n"
     ]
    }
   ],
   "source": [
    "def arpat25_generation(n_rows:int, input_metadata:comp.InputPath('json'), input_preprocess_csv:comp.InputPath('csv'), output_csv:comp.OutputPath('csv')):\n",
    "    import pandas as pd\n",
    "    from synthguard.generate_personal_data import PersonalFaker\n",
    "    from synthguard.synthetic_data_generator import SyntheticDataGenerator\n",
    "    from synthguard.helper_functions import save_to_csv, load_metadata, load_data_csv\n",
    "\n",
    "    N_Rows = n_rows\n",
    "    output_arpat_json_PM25 = 'arpat-synthetic_PM25.csv'\n",
    "\n",
    "    # Create an Italian instance of PersonalFaker\n",
    "    italian_fake = PersonalFaker(\"it_IT\")\n",
    "\n",
    "    it_municipality_codes = italian_fake.generate_administrative_units(N_Rows)\n",
    "    it_municipality_codes\n",
    "\n",
    "    # Generate a list of random dates\n",
    "    random_dates_list = italian_fake.generate_random_dates(N_Rows, \"2023-01-01\", \"2023-12-31\")\n",
    "    random_dates_list\n",
    "\n",
    "    synthetic_arpat_PM25_fake = pd.DataFrame({\n",
    "        'municipality': it_municipality_codes,\n",
    "        'observation_date': random_dates_list\n",
    "    })\n",
    "\n",
    "    processed_data_arpat_PM25 = load_data_csv(input_preprocess_csv)\n",
    "    metadata_arpat_PM25 = load_metadata(input_metadata)\n",
    "\n",
    "\n",
    "    synthetic_arpat_PM25 = SyntheticDataGenerator(n_rows=N_Rows, output_csv=output_arpat_json_PM25, method='realistic').generate_synthetic_data(processed_data_arpat_PM25, metadata_arpat_PM25)\n",
    "    synthetic_arpat_PM25_fake['PM2dot5'] = synthetic_arpat_PM25['PM2dot5']\n",
    "    print(synthetic_arpat_PM25_fake.head())\n",
    "\n",
    "    save_to_csv(synthetic_arpat_PM25_fake, output_csv)\n",
    "\n",
    "arpat25_generation_component = comp.create_component_from_func(arpat25_generation, base_image=BASE_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when stripping type annotations: No module named 'lib2to3'\n"
     ]
    }
   ],
   "source": [
    "def arpat10_generation(n_rows:int, input_metadata:comp.InputPath('json'), input_preprocess_csv:comp.InputPath('csv'), output_csv:comp.OutputPath('csv')):\n",
    "    import pandas as pd\n",
    "    from synthguard.generate_personal_data import PersonalFaker\n",
    "    from synthguard.helper_functions import save_to_csv, load_metadata, load_data_csv\n",
    "    from synthguard.synthetic_data_generator import SyntheticDataGenerator\n",
    "\n",
    "    output_arpat_json_PM10 = 'arpat-synthetic_PM10.csv'\n",
    "\n",
    "    N_Rows = n_rows\n",
    "\n",
    "    # Create an Italian instance of PersonalFaker\n",
    "    italian_fake = PersonalFaker(\"it_IT\")\n",
    "\n",
    "    it_municipality_codes = italian_fake.generate_administrative_units(N_Rows)\n",
    "    it_municipality_codes\n",
    "\n",
    "    # Generate a list of random dates\n",
    "    random_dates_list = italian_fake.generate_random_dates(N_Rows, \"2023-01-01\", \"2023-12-31\")\n",
    "\n",
    "\n",
    "    synthetic_arpat_PM10_fake = pd.DataFrame({\n",
    "        'municipality': it_municipality_codes,\n",
    "        'observation_date': random_dates_list\n",
    "    })\n",
    "\n",
    "    processed_data_arpat_PM10 = load_data_csv(input_preprocess_csv)\n",
    "    metadata_arpat_PM10 = load_metadata(input_metadata)\n",
    "\n",
    "    synthetic_arpat_PM10 = SyntheticDataGenerator(n_rows=N_Rows, output_csv=output_arpat_json_PM10, method='realistic').generate_synthetic_data(processed_data_arpat_PM10, metadata_arpat_PM10)\n",
    "    synthetic_arpat_PM10_fake['PM10'] = synthetic_arpat_PM10['PM10']\n",
    "    synthetic_arpat_PM10_fake\n",
    "\n",
    "    save_to_csv(synthetic_arpat_PM10_fake, output_csv)\n",
    "\n",
    "arpat10_generation_component = comp.create_component_from_func(arpat10_generation, base_image=BASE_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOX2M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# from generate_personal_data import PersonalFaker\n",
    "\n",
    "# # Usage Example\n",
    "# BOX2M_UNITS = [\n",
    "#     {\"name\": \"Current L1\", \"channel\": 8, \"unit\": \"A\"},\n",
    "#     {\"name\": \"Current L2\", \"channel\": 10, \"unit\": \"A\"},\n",
    "#     {\"name\": \"Current L3\", \"channel\": 12, \"unit\": \"A\"},\n",
    "#     {\"name\": \"Total Active Power\", \"channel\": 58, \"unit\": \"KW\"},\n",
    "#     {\"name\": \"Total Active Energy Import\", \"channel\": 6688, \"unit\": \"KWh\"},\n",
    "# ]\n",
    "\n",
    "# box2m_faker = PersonalFaker(locale=\"it_IT\")\n",
    "# output_box2m_file_path = box2m_faker.generate_box2m_data(\n",
    "#     n_addresses=5, \n",
    "#     output_dir=output_path, \n",
    "#     n_records_per_day=3, \n",
    "#     n_box2m_records=500,\n",
    "#     box2m_units=BOX2M_UNITS\n",
    "# )\n",
    "\n",
    "# files_to_be_zipped.append(output_box2m_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when stripping type annotations: No module named 'lib2to3'\n"
     ]
    }
   ],
   "source": [
    "def box2m(output_json:comp.OutputPath('json'), n_records:int = 500):\n",
    "    from synthguard.generate_personal_data import PersonalFaker\n",
    "\n",
    "    # Usage Example\n",
    "    BOX2M_UNITS = [\n",
    "        {\"name\": \"Current L1\", \"channel\": 8, \"unit\": \"A\"},\n",
    "        {\"name\": \"Current L2\", \"channel\": 10, \"unit\": \"A\"},\n",
    "        {\"name\": \"Current L3\", \"channel\": 12, \"unit\": \"A\"},\n",
    "        {\"name\": \"Total Active Power\", \"channel\": 58, \"unit\": \"KW\"},\n",
    "        {\"name\": \"Total Active Energy Import\", \"channel\": 6688, \"unit\": \"KWh\"},\n",
    "    ]\n",
    "\n",
    "    box2m_faker = PersonalFaker(locale=\"it_IT\")\n",
    "    output_box2m_file_path = box2m_faker.generate_box2m_data(\n",
    "        n_addresses=5, \n",
    "        output_dir=output_json, \n",
    "        n_records_per_day=3, \n",
    "        n_box2m_records=n_records,\n",
    "        box2m_units=BOX2M_UNITS\n",
    "    )\n",
    "\n",
    "box2m_component = comp.create_component_from_func(box2m, base_image=BASE_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when stripping type annotations: No module named 'lib2to3'\n"
     ]
    }
   ],
   "source": [
    "def zip_files(rt_cit: comp.InputPath('csv'), \n",
    "              rt_ape: comp.InputPath('zip'), \n",
    "              sir_temp: comp.InputPath('json'), \n",
    "              arpat25: comp.InputPath('csv'), \n",
    "              arpat10: comp.InputPath('csv'),\n",
    "              box2m: comp.InputPath('json'),\n",
    "              output: comp.OutputPath('zip')):\n",
    "    from synthguard.helper_functions import zip_files\n",
    "    \n",
    "    files_to_be_zipped = [\n",
    "        rt_cit,\n",
    "        rt_ape,\n",
    "        sir_temp,\n",
    "        arpat25,\n",
    "        arpat10,\n",
    "        box2m\n",
    "    ]\n",
    "\n",
    "    zip_files(files_to_be_zipped, output)\n",
    "\n",
    "zip_files_component = comp.create_component_from_func(zip_files, base_image=BASE_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name='test_TEADAL_pipeline')\n",
    "def pipeline(n_rows:int):\n",
    "    #PVC init\n",
    "    existing_pvc = dsl.PipelineVolume(pvc='my-pvc')\n",
    "\n",
    "    input_sir = input_component('/mnt/data/datasets/').add_pvolumes({\"/mnt/data/\": existing_pvc})\n",
    "\n",
    "    preprocess_sir = preprocess_comp(input_sir.outputs['output_min_csv'], input_sir.outputs['output_max_csv'])\n",
    "\n",
    "    generation_sir = generation_comp(n_rows,\n",
    "                                 input_sir.outputs['output_min_csv'], \n",
    "                                 input_sir.outputs['output_max_csv'],\n",
    "                                 preprocess_sir.outputs['output_min_metadata'],\n",
    "                                 preprocess_sir.outputs['output_max_metadata'])\n",
    "    \n",
    "    sir_combine = combine_comp(generation_sir.outputs['output_min_csv'], generation_sir.outputs['output_max_csv'], input_sir.outputs['output_json'])\n",
    "    \n",
    "    diagnostic_sir = diagnostic_component(input_sir.outputs['output_min_csv'], \n",
    "                                 input_sir.outputs['output_max_csv'],\n",
    "                                 generation_sir.outputs['output_min_csv'],\n",
    "                                 generation_sir.outputs['output_max_csv'],\n",
    "                                 preprocess_sir.outputs['output_min_metadata'],\n",
    "                                 preprocess_sir.outputs['output_max_metadata'])\n",
    "    \n",
    "    utility_sir = quality_comp(input_sir.outputs['output_min_csv'], \n",
    "                                 input_sir.outputs['output_max_csv'],\n",
    "                                 generation_sir.outputs['output_min_csv'],\n",
    "                                 generation_sir.outputs['output_max_csv'],\n",
    "                                 preprocess_sir.outputs['output_min_metadata'],\n",
    "                                 preprocess_sir.outputs['output_max_metadata'])\n",
    "    \n",
    "    privacy_sir = privacy_comp(input_sir.outputs['output_min_csv'], \n",
    "                                 input_sir.outputs['output_max_csv'],\n",
    "                                 generation_sir.outputs['output_min_csv'],\n",
    "                                 generation_sir.outputs['output_max_csv'],\n",
    "                                 preprocess_sir.outputs['output_min_metadata'],\n",
    "                                 preprocess_sir.outputs['output_max_metadata'])\n",
    "    \n",
    "    load_streets_and_municipalities = load_streets_and_municipalities_comp('/mnt/data/datasets/').add_pvolumes({\"/mnt/data/\": existing_pvc})\n",
    "\n",
    "    address_generation = address_generation_component(n_rows, load_streets_and_municipalities.output)\n",
    "\n",
    "    rtcit = rtcit_component(address_generation.output)\n",
    "\n",
    "    rtape = rtape_component('/mnt/data/datasets/', rtcit.output).add_pvolumes({\"/mnt/data/\": existing_pvc})\n",
    "    \n",
    "    load_arpat = load_arpat_component('/mnt/data/datasets').add_pvolumes({\"/mnt/data/\": existing_pvc})\n",
    "\n",
    "    arpat_preprocessor = arpat_preprocessor_component(load_arpat.outputs['output_pm10_csv'], load_arpat.outputs['output_pm25_csv'])\n",
    "\n",
    "    arpat25_generation = arpat25_generation_component(n_rows, arpat_preprocessor.outputs['output_pm25_metadata'], arpat_preprocessor.outputs['output_pm25_csv'])\n",
    "\n",
    "    arpat10_generation = arpat10_generation_component(n_rows, arpat_preprocessor.outputs['output_pm10_metadata'], arpat_preprocessor.outputs['output_pm10_csv'])\n",
    "\n",
    "    box2m = box2m_component()\n",
    "\n",
    "    zip_files = zip_files_component(rtcit.output,\n",
    "                          rtape.output,\n",
    "                          sir_combine.outputs['output_combined_json'],\n",
    "                          arpat25_generation.output,\n",
    "                          arpat10_generation.output,\n",
    "                          box2m.output)\n",
    "        \n",
    "Compiler().compile(pipeline, 'TEADAL.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import Client\n",
    "from kubernetes import client as k8s_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load kube config.\n"
     ]
    },
    {
     "ename": "MaxRetryError",
     "evalue": "HTTPConnectionPool(host='localhost', port=80): Max retries exceeded with url: /apis/v1beta1/healthz (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x72d25a978ef0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m~/data-synthesis-workflow-engine/.venv/lib/python3.12/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[0;32m~/data-synthesis-workflow-engine/.venv/lib/python3.12/site-packages/urllib3/util/connection.py:95\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetaddrinfo returns an empty list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/data-synthesis-workflow-engine/.venv/lib/python3.12/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 85\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sock\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/data-synthesis-workflow-engine/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:716\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 716\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/data-synthesis-workflow-engine/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:416\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m         \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhttplib_request_kw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m~/data-synthesis-workflow-engine/.venv/lib/python3.12/site-packages/urllib3/connection.py:244\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m    243\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_default_user_agent()\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHTTPConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/http/client.py:1336\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1336\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/http/client.py:1382\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1381\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1382\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/http/client.py:1331\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1331\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/http/client.py:1091\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1091\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1094\u001b[0m \n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/http/client.py:1035\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m-> 1035\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/data-synthesis-workflow-engine/.venv/lib/python3.12/site-packages/urllib3/connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 205\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_conn(conn)\n",
      "File \u001b[0;32m~/data-synthesis-workflow-engine/.venv/lib/python3.12/site-packages/urllib3/connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m e\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conn\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x72d25a978ef0>: Failed to establish a new connection: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Compile the pipeline with the configuration\u001b[39;00m\n\u001b[1;32m      5\u001b[0m pipeline_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTEADAL_IMAGE.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m run_result \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mcreate_run_from_pipeline_func(pipeline, arguments\u001b[38;5;241m=\u001b[39m{}, pipeline_conf\u001b[38;5;241m=\u001b[39mpipeline_conf)\n",
      "File \u001b[0;32m~/data-synthesis-workflow-engine/.venv/lib/python3.12/site-packages/kfp/_client.py:196\u001b[0m, in \u001b[0;36mClient.__init__\u001b[0;34m(self, host, client_id, namespace, other_client_id, other_client_secret, existing_token, cookies, proxy, ssl_ca_cert, kube_context, credentials, ui_host)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upload_api \u001b[38;5;241m=\u001b[39m kfp_server_api\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mPipelineUploadServiceApi(\n\u001b[1;32m    193\u001b[0m     api_client)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_healthz_api \u001b[38;5;241m=\u001b[39m kfp_server_api\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mhealthz_service_api\u001b[38;5;241m.\u001b[39mHealthzServiceApi(\n\u001b[1;32m    195\u001b[0m     api_client)\n\u001b[0;32m--> 196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context_setting[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_kfp_healthz\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmulti_user \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(Client\u001b[38;5;241m.\u001b[39mNAMESPACE_PATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/data-synthesis-workflow-engine/.venv/lib/python3.12/site-packages/kfp/_client.py:410\u001b[0m, in \u001b[0;36mClient.get_kfp_healthz\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed getting healthz endpoint after \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m attempts.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    408\u001b[0m             max_attempts))\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 410\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_healthz_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_healthz\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    412\u001b[0m \u001b[38;5;66;03m# ApiException, including network errors, is the only type that may\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# recover after retry.\u001b[39;00m\n",
      "File \u001b[0;32m~/data-synthesis-workflow-engine/.venv/lib/python3.12/site-packages/kfp_server_api/api/healthz_service_api.py:63\u001b[0m, in \u001b[0;36mHealthzServiceApi.get_healthz\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get healthz data.  # noqa: E501\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03mThis method makes a synchronous HTTP request by default. To make an\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m:rtype: ApiGetHealthzResponse\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_return_http_data_only\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_healthz_with_http_info\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data-synthesis-workflow-engine/.venv/lib/python3.12/site-packages/kfp_server_api/api/healthz_service_api.py:134\u001b[0m, in \u001b[0;36mHealthzServiceApi.get_healthz_with_http_info\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Authentication setting\u001b[39;00m\n\u001b[1;32m    132\u001b[0m auth_settings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBearer\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/apis/v1beta1/healthz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mform_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mApiGetHealthzResponse\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# noqa: E501\u001b[39;49;00m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_req\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43masync_req\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_return_http_data_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_return_http_data_only\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# noqa: E501\u001b[39;49;00m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_preload_content\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_var_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_request_timeout\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_formats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_formats\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data-synthesis-workflow-engine/.venv/lib/python3.12/site-packages/kfp_server_api/api_client.py:364\u001b[0m, in \u001b[0;36mApiClient.call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Makes the HTTP request (synchronous) and returns deserialized data.\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03mTo make an async_req request, set the async_req parameter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    then the method will return the response directly.\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m async_req:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__call_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mresponse_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m                           \u001b[49m\u001b[43m_return_http_data_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection_formats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m                           \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_host\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mapply_async(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__call_api, (resource_path,\n\u001b[1;32m    372\u001b[0m                                                method, path_params,\n\u001b[1;32m    373\u001b[0m                                                query_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    381\u001b[0m                                                _request_timeout,\n\u001b[1;32m    382\u001b[0m                                                _host))\n",
      "File \u001b[0;32m~/data-synthesis-workflow-engine/.venv/lib/python3.12/site-packages/kfp_server_api/api_client.py:181\u001b[0m, in \u001b[0;36mApiClient.__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host)\u001b[0m\n\u001b[1;32m    177\u001b[0m     url \u001b[38;5;241m=\u001b[39m _host \u001b[38;5;241m+\u001b[39m resource_path\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# perform request and return response\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m     response_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ApiException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    187\u001b[0m     e\u001b[38;5;241m.\u001b[39mbody \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mbody\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m six\u001b[38;5;241m.\u001b[39mPY3 \u001b[38;5;28;01melse\u001b[39;00m e\u001b[38;5;241m.\u001b[39mbody\n",
      "File \u001b[0;32m~/data-synthesis-workflow-engine/.venv/lib/python3.12/site-packages/kfp_server_api/api_client.py:389\u001b[0m, in \u001b[0;36mApiClient.request\u001b[0;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Makes the HTTP request using RESTClient.\"\"\"\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrest_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m                                \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m                                \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrest_client\u001b[38;5;241m.\u001b[39mHEAD(url,\n\u001b[1;32m    396\u001b[0m                                  query_params\u001b[38;5;241m=\u001b[39mquery_params,\n\u001b[1;32m    397\u001b[0m                                  _preload_content\u001b[38;5;241m=\u001b[39m_preload_content,\n\u001b[1;32m    398\u001b[0m                                  _request_timeout\u001b[38;5;241m=\u001b[39m_request_timeout,\n\u001b[1;32m    399\u001b[0m                                  headers\u001b[38;5;241m=\u001b[39mheaders)\n",
      "File \u001b[0;32m~/data-synthesis-workflow-engine/.venv/lib/python3.12/site-packages/kfp_server_api/rest.py:230\u001b[0m, in \u001b[0;36mRESTClientObject.GET\u001b[0;34m(self, url, headers, query_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mGET\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, query_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, _preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    229\u001b[0m         _request_timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data-synthesis-workflow-engine/.venv/lib/python3.12/site-packages/kfp_server_api/rest.py:208\u001b[0m, in \u001b[0;36mRESTClientObject.request\u001b[0;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m ApiException(status\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, reason\u001b[38;5;241m=\u001b[39mmsg)\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;66;03m# For `GET`, `HEAD`\u001b[39;00m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m         r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m urllib3\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mSSLError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    214\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(e)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[0;32m~/data-synthesis-workflow-engine/.venv/lib/python3.12/site-packages/urllib3/request.py:77\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m     74\u001b[0m urlopen_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest_url\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m url\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_url_methods:\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_encode_url\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43murlopen_kw\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_body(\n\u001b[1;32m     82\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[1;32m     83\u001b[0m     )\n",
      "File \u001b[0;32m~/data-synthesis-workflow-engine/.venv/lib/python3.12/site-packages/urllib3/request.py:99\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_url\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fields:\n\u001b[1;32m     97\u001b[0m     url \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m urlencode(fields)\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data-synthesis-workflow-engine/.venv/lib/python3.12/site-packages/urllib3/poolmanager.py:376\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    374\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m~/data-synthesis-workflow-engine/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:830\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn:\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;66;03m# Try again\u001b[39;00m\n\u001b[1;32m    827\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    828\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[1;32m    829\u001b[0m     )\n\u001b[0;32m--> 830\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n\u001b[1;32m    847\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n",
      "File \u001b[0;32m~/data-synthesis-workflow-engine/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:830\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn:\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;66;03m# Try again\u001b[39;00m\n\u001b[1;32m    827\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    828\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[1;32m    829\u001b[0m     )\n\u001b[0;32m--> 830\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n\u001b[1;32m    847\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n",
      "File \u001b[0;32m~/data-synthesis-workflow-engine/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:830\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn:\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;66;03m# Try again\u001b[39;00m\n\u001b[1;32m    827\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    828\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[1;32m    829\u001b[0m     )\n\u001b[0;32m--> 830\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n\u001b[1;32m    847\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n",
      "File \u001b[0;32m~/data-synthesis-workflow-engine/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:802\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, (SocketError, HTTPException)):\n\u001b[1;32m    800\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m--> 802\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# Keep track of the error for the retry warning.\u001b[39;00m\n",
      "File \u001b[0;32m~/data-synthesis-workflow-engine/.venv/lib/python3.12/site-packages/urllib3/util/retry.py:594\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    583\u001b[0m new_retry \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew(\n\u001b[1;32m    584\u001b[0m     total\u001b[38;5;241m=\u001b[39mtotal,\n\u001b[1;32m    585\u001b[0m     connect\u001b[38;5;241m=\u001b[39mconnect,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    590\u001b[0m     history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[1;32m    591\u001b[0m )\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 594\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    596\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_retry\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=80): Max retries exceeded with url: /apis/v1beta1/healthz (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x72d25a978ef0>: Failed to establish a new connection: [Errno 111] Connection refused'))"
     ]
    }
   ],
   "source": [
    "pipeline_conf = dsl.PipelineConf()\n",
    "pipeline_conf.set_image_pull_secrets([k8s_client.V1ObjectReference(name=\"regcred\")])\n",
    "\n",
    "# Compile the pipeline with the configuration\n",
    "pipeline_path = 'TEADAL_IMAGE.yaml'\n",
    "client = Client()\n",
    "run_result = client.create_run_from_pipeline_func(pipeline, arguments={}, pipeline_conf=pipeline_conf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
