{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp.compiler import Compiler\n",
    "from kfp import components as comp\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies = ['git+https://mshoush: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@github.com/mshoush/synthguard.git']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IMAGE = 'gitlab.ext.cyber.ee:5050/exai/synthguard:latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "\n",
    "# from helper_functions import load_json, handle_nested_data_json, reverse_flatten\n",
    "# import pandas as pd\n",
    "\n",
    "# files_to_be_zipped = []\n",
    "\n",
    "# input_path = \"./../../data-synthesis/docs/examples/energy-pilot-teadal/datasets/\"\n",
    "# output_path = 'synthetic_datasets/teadal/'\n",
    "\n",
    "# file1 = 'sir-min-temp.json'\n",
    "# file2 = 'sir-max-temp.json'\n",
    "\n",
    "# real_data1 = handle_nested_data_json(pd.json_normalize(load_json(input_path + file1)))\n",
    "# real_data2 = handle_nested_data_json(pd.json_normalize(load_json(input_path + file2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sir_input(input_path:str,\n",
    "          output_min_csv: comp.OutputPath('csv'), \n",
    "          output_max_csv: comp.OutputPath('csv'),\n",
    "          output_json: comp.OutputPath('json')):\n",
    "    from synthguard.helper_functions import load_json, handle_nested_data_json, save_to_csv, save_json\n",
    "    import pandas as pd\n",
    "\n",
    "    file1 = 'sir-min-temp.json'\n",
    "    file2 = 'sir-max-temp.json'\n",
    "\n",
    "    real_data1 = handle_nested_data_json(pd.json_normalize(load_json(input_path + file1)))\n",
    "    real_data2 = handle_nested_data_json(pd.json_normalize(load_json(input_path + file2)))\n",
    "\n",
    "    save_to_csv(real_data1, output_min_csv)\n",
    "    save_to_csv(real_data2, output_max_csv)\n",
    "    save_json(output_json, file1, load_json(input_path + file1))\n",
    "\n",
    "input_component = comp.create_component_from_func(sir_input, base_image=BASE_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data_preprocessor import DataPreprocessor \n",
    "\n",
    "# dataPreprocessor1 = DataPreprocessor(data = real_data1)\n",
    "# processed_data1, metadata1 = dataPreprocessor1.preprocess_data()\n",
    "\n",
    "# dataPreprocessor2 = DataPreprocessor(data = real_data2)\n",
    "# processed_data2, metadata2 = dataPreprocessor2.preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sir_preprocess(input_min_csv: comp.InputPath('csv'), \n",
    "               input_max_csv: comp.InputPath('csv'),\n",
    "               output_min_metadata: comp.OutputPath('json'),\n",
    "               output_max_metadata: comp.OutputPath('json')):\n",
    "    from synthguard.data_preprocessor import DataPreprocessor\n",
    "    from synthguard.helper_functions import save_metadata, load_data_csv\n",
    "\n",
    "    real_data1 = load_data_csv(input_min_csv)\n",
    "    real_data2 = load_data_csv(input_max_csv)\n",
    "\n",
    "    dataPreprocessor1 = DataPreprocessor(data = real_data1)\n",
    "    processed_data1, metadata1 = dataPreprocessor1.preprocess_data()\n",
    "    save_metadata(metadata1, output_min_metadata)\n",
    "\n",
    "    dataPreprocessor2 = DataPreprocessor(data = real_data2)\n",
    "    processed_data2, metadata2 = dataPreprocessor2.preprocess_data()\n",
    "    save_metadata(metadata2, output_max_metadata)\n",
    "\n",
    "preprocess_comp = comp.create_component_from_func(sir_preprocess, base_image=BASE_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from synthetic_data_generator import SyntheticDataGenerator\n",
    "\n",
    "\n",
    "# output1_json = 'sir-min-temp-synthetic.csv'\n",
    "# output2_json = 'sir-max-temp-synthetic.csv'\n",
    "\n",
    "\n",
    "# if output1_json:\n",
    "#     # Create the output path if it does not exist\n",
    "#     import os\n",
    "#     if not os.path.exists(output_path):\n",
    "#         os.makedirs(output_path)\n",
    "#     output1 = output_path + output1_json\n",
    "\n",
    "\n",
    "# if output2_json:\n",
    "#     # Create the output path if it does not exist\n",
    "#     import os\n",
    "#     if not os.path.exists(output_path):\n",
    "#         os.makedirs(output_path)\n",
    "#     output2 = output_path + output1_json\n",
    "\n",
    "\n",
    "# N_Rows = 1000\n",
    "# EPOCHS = 1\n",
    "# Locales = 'ee_ET'\n",
    "# synthetic_data_type = 'realistic'\n",
    "\n",
    "# syntheticDataGenerator1 = SyntheticDataGenerator(locales=Locales, n_rows=N_Rows, method=synthetic_data_type, output_csv=output1)\n",
    "# generated_data1 = syntheticDataGenerator1.generate_synthetic_data(metadata = metadata1, processed_data = processed_data1, Nepochs=EPOCHS)\n",
    "\n",
    "# syntheticDataGenerator2 = SyntheticDataGenerator(locales=Locales, n_rows=N_Rows, method=synthetic_data_type, output_csv=output2)\n",
    "# generated_data2 = syntheticDataGenerator2.generate_synthetic_data(metadata = metadata2, processed_data = processed_data2, Nepochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sir_generation(n_rows: int,\n",
    "               input_min_csv: comp.InputPath('csv'),\n",
    "               input_max_csv: comp.InputPath('csv'),\n",
    "               input_min_metadata: comp.InputPath('json'),\n",
    "               input_max_metadata: comp.InputPath('json'),\n",
    "               output_min_csv:comp.OutputPath('csv'),\n",
    "               output_max_csv:comp.OutputPath('csv')):\n",
    "    from synthguard.synthetic_data_generator import SyntheticDataGenerator\n",
    "    from synthguard.helper_functions import load_metadata, load_data_csv, save_to_csv\n",
    "\n",
    "    metadata1 = load_metadata(input_min_metadata)\n",
    "    metadata2 = load_metadata(input_max_metadata)\n",
    "\n",
    "    processed_data1 =  load_data_csv(input_min_csv)\n",
    "    processed_data2 = load_data_csv(input_max_csv)\n",
    "\n",
    "    N_Rows = n_rows\n",
    "    EPOCHS = 1\n",
    "    Locales = 'ee_ET'\n",
    "    synthetic_data_type = 'realistic'\n",
    "\n",
    "    syntheticDataGenerator1 = SyntheticDataGenerator(locales=Locales, n_rows=N_Rows, method=synthetic_data_type, output_csv=output_min_csv)\n",
    "    generated_data1 = syntheticDataGenerator1.generate_synthetic_data(metadata = metadata1, processed_data = processed_data1, Nepochs=EPOCHS)\n",
    "\n",
    "    syntheticDataGenerator2 = SyntheticDataGenerator(locales=Locales, n_rows=N_Rows, method=synthetic_data_type, output_csv=output_max_csv)\n",
    "    generated_data2 = syntheticDataGenerator2.generate_synthetic_data(metadata = metadata2, processed_data = processed_data2, Nepochs=EPOCHS)\n",
    "\n",
    "generation_comp = comp.create_component_from_func(sir_generation, base_image=BASE_IMAGE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sir_combine(input_min_csv: comp.InputPath('csv'),\n",
    "                input_max_csv: comp.InputPath('csv'),\n",
    "                input_json: comp.InputPath('json'),\n",
    "                output_combined_json: comp.OutputPath('json')):\n",
    "    import os\n",
    "    from synthguard.helper_functions import reverse_flatten, save_json, load_data_csv, load_json\n",
    "\n",
    "    # Debug input and output paths\n",
    "    input_json_path = os.path.join(input_json, 'sir-min-temp.json')\n",
    "    output_combined = 'sir-min-max-temp-synthetic.json'\n",
    "    \n",
    "    print(f\"input_min_csv path: {input_min_csv}\")\n",
    "    print(f\"input_max_csv path: {input_max_csv}\")\n",
    "    print(f\"input_json path: {input_json_path}\")\n",
    "    print(f\"output_combined_json path: {output_combined_json}\")\n",
    "    \n",
    "    # Load CSV data and original JSON structure\n",
    "    generated_data1 = load_data_csv(input_min_csv)\n",
    "    generated_data2 = load_data_csv(input_max_csv)\n",
    "    original_json_structure = load_json(input_json_path)\n",
    "\n",
    "    # Validate that the JSON structure is a dictionary\n",
    "    if not isinstance(original_json_structure, dict):\n",
    "        raise ValueError(f\"Expected a dictionary from {input_json_path}, but got {type(original_json_structure)}\")\n",
    "\n",
    "    # Reverse-flatten the generated data using the original JSON structure\n",
    "    rebuilt_data1 = reverse_flatten(generated_data1, original_json_structure)\n",
    "    rebuilt_data2 = reverse_flatten(generated_data2, original_json_structure)\n",
    "\n",
    "    # Combine the data\n",
    "    combined_data = {0: rebuilt_data1, 1: rebuilt_data2}\n",
    "\n",
    "    # Ensure output directory exists and save the combined JSON\n",
    "    os.makedirs(os.path.dirname(output_combined_json), exist_ok=True)\n",
    "    save_json(output_combined_json, output_combined, combined_data)\n",
    "\n",
    "    print(f\"Merged JSON saved to {os.path.join(output_combined_json, output_combined)}\")\n",
    "    \n",
    "# Define the component\n",
    "combine_comp = comp.create_component_from_func(sir_combine, base_image=BASE_IMAGE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnostic Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from diagnostic_report_generator import DiagnosticEvaluator\n",
    "\n",
    "# diagnosticReportGenerator1 = DiagnosticEvaluator(real_data = processed_data1, synthetic_data = generated_data1, metadata = metadata1, method=synthetic_data_type)\n",
    "# diagnosticReportGenerator1.run_diagnostic_realistic()\n",
    "# diagnosticReportGenerator1.plot_diagnostic_report_realistic(output_path = output_path)\n",
    "\n",
    "# diagnosticReportGenerator2 = DiagnosticEvaluator(real_data = processed_data2, synthetic_data = generated_data2, metadata = metadata2, method=synthetic_data_type)\n",
    "# diagnosticReportGenerator2.run_diagnostic_realistic()\n",
    "# diagnosticReportGenerator2.plot_diagnostic_report_realistic(output_path = output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sir_diagnostic(input_min_real_csv: comp.InputPath('csv'),\n",
    "               input_max_real_csv: comp.InputPath('csv'),\n",
    "               input_min_synth_csv: comp.InputPath('csv'),\n",
    "               input_max_synth_csv: comp.InputPath('csv'),\n",
    "               input_min_metadata: comp.InputPath('json'),\n",
    "               input_max_metadata: comp.InputPath('json'),\n",
    "               output_html: comp.OutputPath('html')) -> NamedTuple('VisualizationOutput', [('mlpipeline_ui_metadata', 'UI_metadata')]):\n",
    "    from synthguard.diagnostic_report_generator import DiagnosticEvaluator\n",
    "    from synthguard.helper_functions import load_data_csv, load_metadata\n",
    "    import json\n",
    "\n",
    "    with open(output_html, \"w\") as f:\n",
    "        f.write(\"<html><body>\")  # Start the HTML document\n",
    "\n",
    "    processed_data1 =  load_data_csv(input_min_real_csv)\n",
    "    processed_data2 = load_data_csv(input_max_real_csv)\n",
    "\n",
    "    generated_data1 = load_data_csv(input_min_synth_csv)\n",
    "    generated_data2 = load_data_csv(input_max_synth_csv)\n",
    "\n",
    "    metadata1 = load_metadata(input_min_metadata)\n",
    "    metadata2 = load_metadata(input_max_metadata)\n",
    "\n",
    "    synthetic_data_type = 'realistic'\n",
    "\n",
    "    diagnosticReportGenerator1 = DiagnosticEvaluator(real_data = processed_data1, synthetic_data = generated_data1, metadata = metadata1, method=synthetic_data_type)\n",
    "    diagnosticReportGenerator1.run_diagnostic_realistic()\n",
    "    diagnosticReportGenerator1.plot_diagnostic_report_realistic()\n",
    "    diagnosticReportGenerator1.save_plot_to_html(output_html)\n",
    "\n",
    "    diagnosticReportGenerator2 = DiagnosticEvaluator(real_data = processed_data2, synthetic_data = generated_data2, metadata = metadata2, method=synthetic_data_type)\n",
    "    diagnosticReportGenerator2.run_diagnostic_realistic()\n",
    "    diagnosticReportGenerator2.plot_diagnostic_report_realistic()\n",
    "    diagnosticReportGenerator2.save_plot_to_html(output_html)\n",
    "\n",
    "    # Read the HTML content for UI metadata\n",
    "    with open(output_html, 'r') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    metadata = {\n",
    "        'outputs': [{\n",
    "            'type': 'web-app',\n",
    "            'storage': 'inline',\n",
    "            'source': html_content,\n",
    "        }]\n",
    "    }\n",
    "\n",
    "    from collections import namedtuple\n",
    "    visualization_output = namedtuple('VisualizationOutput', ['mlpipeline_ui_metadata'])\n",
    "    return visualization_output(json.dumps(metadata))\n",
    "\n",
    "diagnostic_component = comp.create_component_from_func(sir_diagnostic, base_image=BASE_IMAGE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from synthguard.quality_report_generator import DataQualityEvaluator\n",
    "\n",
    "# dataQualityEvaluator1 = DataQualityEvaluator(real_data = processed_data1, synthetic_data = generated_data1, metadata = metadata1, method=synthetic_data_type)\n",
    "# dataQualityEvaluator1.evaluate_quality()\n",
    "# dataQualityEvaluator1.plot_quality_report_realistic(output_path = output_path)\n",
    "\n",
    "\n",
    "# dataQualityEvaluator2 = DataQualityEvaluator(real_data = processed_data2, synthetic_data = generated_data2, metadata = metadata2, method=synthetic_data_type)\n",
    "# dataQualityEvaluator2.evaluate_quality()\n",
    "# dataQualityEvaluator2.plot_quality_report_realistic(output_path = output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sir_quality(input_min_real_csv: comp.InputPath('csv'),\n",
    "               input_max_real_csv: comp.InputPath('csv'),\n",
    "               input_min_synth_csv: comp.InputPath('csv'),\n",
    "               input_max_synth_csv: comp.InputPath('csv'),\n",
    "               input_min_metadata: comp.InputPath('json'),\n",
    "               input_max_metadata: comp.InputPath('json'),\n",
    "               output_html: comp.OutputPath('html')) -> NamedTuple('VisualizationOutput', [('mlpipeline_ui_metadata', 'UI_metadata')]):\n",
    "    from synthguard.quality_report_generator import DataQualityEvaluator\n",
    "    from synthguard.helper_functions import load_data_csv, load_metadata\n",
    "    import json\n",
    "\n",
    "    with open(output_html, \"w\") as f:\n",
    "        f.write(\"<html><body>\")  # Start the HTML document\n",
    "\n",
    "    processed_data1 =  load_data_csv(input_min_real_csv)\n",
    "    processed_data2 = load_data_csv(input_max_real_csv)\n",
    "\n",
    "    generated_data1 = load_data_csv(input_min_synth_csv)\n",
    "    generated_data2 = load_data_csv(input_max_synth_csv)\n",
    "\n",
    "    metadata1 = load_metadata(input_min_metadata)\n",
    "    metadata2 = load_metadata(input_max_metadata)\n",
    "\n",
    "    synthetic_data_type = 'realistic'\n",
    "\n",
    "    dataQualityEvaluator1 = DataQualityEvaluator(real_data = processed_data1, synthetic_data = generated_data1, metadata = metadata1, method=synthetic_data_type)\n",
    "    dataQualityEvaluator1.evaluate_quality()\n",
    "    dataQualityEvaluator1.plot_quality_report_realistic()\n",
    "    dataQualityEvaluator1.save_plot_to_html(output_html)\n",
    "\n",
    "    dataQualityEvaluator2 = DataQualityEvaluator(real_data = processed_data2, synthetic_data = generated_data2, metadata = metadata2, method=synthetic_data_type)\n",
    "    dataQualityEvaluator2.evaluate_quality()\n",
    "    dataQualityEvaluator2.plot_quality_report_realistic()\n",
    "    dataQualityEvaluator2.save_plot_to_html(output_html)\n",
    "\n",
    "    # Read the HTML content for UI metadata\n",
    "    with open(output_html, 'r') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    metadata = {\n",
    "        'outputs': [{\n",
    "            'type': 'web-app',\n",
    "            'storage': 'inline',\n",
    "            'source': html_content,\n",
    "        }]\n",
    "    }\n",
    "\n",
    "    from collections import namedtuple\n",
    "    visualization_output = namedtuple('VisualizationOutput', ['mlpipeline_ui_metadata'])\n",
    "    return visualization_output(json.dumps(metadata))\n",
    "\n",
    "quality_comp = comp.create_component_from_func(sir_quality, base_image=BASE_IMAGE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Privacy Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from privacy_report_generator import PrivacyRiskEvaluator\n",
    "\n",
    "# privacyRiskEvaluator1 = PrivacyRiskEvaluator(real_data = processed_data1, synthetic_data = generated_data1, metadata = metadata1, method=synthetic_data_type)\n",
    "# privacyRiskEvaluator1.run_privacy_realistic()\n",
    "# privacyRiskEvaluator1.plot_privacy_metrics_realistic(output_path = output_path)\n",
    "\n",
    "# privacyRiskEvaluator2 = PrivacyRiskEvaluator(real_data = processed_data2, synthetic_data = generated_data2, metadata = metadata2, method=synthetic_data_type)       \n",
    "# privacyRiskEvaluator2.run_privacy_realistic()\n",
    "# privacyRiskEvaluator2.plot_privacy_metrics_realistic(output_path = output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sir_privacy(input_min_real_csv: comp.InputPath('csv'),\n",
    "               input_max_real_csv: comp.InputPath('csv'),\n",
    "               input_min_synth_csv: comp.InputPath('csv'),\n",
    "               input_max_synth_csv: comp.InputPath('csv'),\n",
    "               input_min_metadata: comp.InputPath('json'),\n",
    "               input_max_metadata: comp.InputPath('json'),\n",
    "               output_html: comp.OutputPath('html')) -> NamedTuple('VisualizationOutput', [('mlpipeline_ui_metadata', 'UI_metadata')]):\n",
    "    from synthguard.privacy_report_generator import PrivacyRiskEvaluator\n",
    "    from synthguard.helper_functions import load_data_csv, load_metadata\n",
    "    import json\n",
    "\n",
    "    with open(output_html, \"w\") as f:\n",
    "        f.write(\"<html><body>\")  # Start the HTML document\n",
    "\n",
    "    processed_data1 =  load_data_csv(input_min_real_csv)\n",
    "    processed_data2 = load_data_csv(input_max_real_csv)\n",
    "\n",
    "    generated_data1 = load_data_csv(input_min_synth_csv)\n",
    "    generated_data2 = load_data_csv(input_max_synth_csv)\n",
    "\n",
    "    metadata1 = load_metadata(input_min_metadata)\n",
    "    metadata2 = load_metadata(input_max_metadata)\n",
    "\n",
    "    synthetic_data_type = 'realistic'    \n",
    "\n",
    "    privacyRiskEvaluator1 = PrivacyRiskEvaluator(real_data = processed_data1, synthetic_data = generated_data1, metadata = metadata1, method=synthetic_data_type)\n",
    "    privacyRiskEvaluator1.run_privacy_realistic()\n",
    "    privacyRiskEvaluator1.plot_privacy_metrics_realistic()\n",
    "    privacyRiskEvaluator1.save_plot_to_html(output_html)\n",
    "\n",
    "    privacyRiskEvaluator2 = PrivacyRiskEvaluator(real_data = processed_data2, synthetic_data = generated_data2, metadata = metadata2, method=synthetic_data_type)       \n",
    "    privacyRiskEvaluator2.run_privacy_realistic()\n",
    "    privacyRiskEvaluator2.plot_privacy_metrics_realistic()\n",
    "    privacyRiskEvaluator2.save_plot_to_html(output_html)\n",
    "        # Read the HTML content for UI metadata\n",
    "    with open(output_html, 'r') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    metadata = {\n",
    "        'outputs': [{\n",
    "            'type': 'web-app',\n",
    "            'storage': 'inline',\n",
    "            'source': html_content,\n",
    "        }]\n",
    "    }\n",
    "\n",
    "    from collections import namedtuple\n",
    "    visualization_output = namedtuple('VisualizationOutput', ['mlpipeline_ui_metadata'])\n",
    "    return visualization_output(json.dumps(metadata))\n",
    "\n",
    "privacy_comp = comp.create_component_from_func(sir_privacy, base_image=BASE_IMAGE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEADAL half pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsl.pipeline(name='TEADAL_half_pipeline')\n",
    "def pipeline(n_rows:int):\n",
    "    #PVC init\n",
    "    existing_pvc = dsl.PipelineVolume(pvc='my-pvc')\n",
    "\n",
    "    input_sir = input_component('/mnt/pvc/').add_pvolumes({\"/mnt/pvc\": existing_pvc})\n",
    "\n",
    "    preprocess_sir = preprocess_comp(input_sir.outputs['output_min_csv'], input_sir.outputs['output_max_csv'])\n",
    "\n",
    "    generation_sir = generation_comp(n_rows,\n",
    "                                 input_sir.outputs['output_min_csv'], \n",
    "                                 input_sir.outputs['output_max_csv'],\n",
    "                                 preprocess_sir.outputs['output_min_metadata'],\n",
    "                                 preprocess_sir.outputs['output_max_metadata'])\n",
    "    \n",
    "    diagnostic_sir = diagnostic_component(input_sir.outputs['output_min_csv'], \n",
    "                                 input_sir.outputs['output_max_csv'],\n",
    "                                 generation_sir.outputs['output_min_csv'],\n",
    "                                 generation_sir.outputs['output_max_csv'],\n",
    "                                 preprocess_sir.outputs['output_min_metadata'],\n",
    "                                 preprocess_sir.outputs['output_max_metadata'])\n",
    "    \n",
    "    utility_sir = quality_comp(input_sir.outputs['output_min_csv'], \n",
    "                                 input_sir.outputs['output_max_csv'],\n",
    "                                 generation_sir.outputs['output_min_csv'],\n",
    "                                 generation_sir.outputs['output_max_csv'],\n",
    "                                 preprocess_sir.outputs['output_min_metadata'],\n",
    "                                 preprocess_sir.outputs['output_max_metadata'])\n",
    "    \n",
    "    privacy_sir = privacy_comp(input_sir.outputs['output_min_csv'], \n",
    "                                 input_sir.outputs['output_max_csv'],\n",
    "                                 generation_sir.outputs['output_min_csv'],\n",
    "                                 generation_sir.outputs['output_max_csv'],\n",
    "                                 preprocess_sir.outputs['output_min_metadata'],\n",
    "                                 preprocess_sir.outputs['output_max_metadata'])\n",
    "    \n",
    "Compiler().compile(pipeline, 'teadal_half_pipeline.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text files street_names and municipalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from faker import Faker\n",
    "# import pandas as pd\n",
    "# from generate_personal_data import PersonalFaker\n",
    "# from helper_functions import read_txt_and_convert_to_df\n",
    "\n",
    "\n",
    "\n",
    "# # Create an instance of PersonalFaker\n",
    "# estonian_fake = PersonalFaker(\"it_IT\")\n",
    "\n",
    "# # input_path = \"./../../data-synthesis/docs/examples/energy-pilot-teadal/datasets/\"\n",
    "\n",
    "# input_file1 = \"street_names.txt\"\n",
    "# input_file2 = \"municipality_codes.txt\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Read and truncate files to the same length\n",
    "# street_names, municipality_codes = (read_txt_and_convert_to_df(input_path, f) for f in [input_file1, input_file2])\n",
    "# min_rows = min(len(street_names), len(municipality_codes))\n",
    "\n",
    "# # Create the DataFrame with 'id' and 'address'\n",
    "# real_data_addresses = pd.DataFrame({\n",
    "#     'id': range(1, min_rows + 1),\n",
    "#     'address': street_names.iloc[:min_rows].squeeze() + \", \" + municipality_codes.iloc[:min_rows].squeeze().astype(str)\n",
    "# })\n",
    "\n",
    "# real_data_addresses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_streets_and_municipalities(input_path:str)->dict:\n",
    "    from synthguard.helper_functions import read_txt_and_convert_to_df\n",
    "\n",
    "    input_file1 = input_path+\"street_names.txt\"\n",
    "    input_file2 = input_path+\"municipality_codes.txt\"\n",
    "\n",
    "    street_names, municipality_codes = (read_txt_and_convert_to_df(input_path, f) for f in [input_file1, input_file2])\n",
    "\n",
    "    print(street_names.head())\n",
    "    print(municipality_codes.head())\n",
    "    \n",
    "    streets_and_municipalities = {\n",
    "        'streets': street_names.values.tolist(),\n",
    "        'municipalities': municipality_codes.values.tolist()\n",
    "    }\n",
    "    return streets_and_municipalities\n",
    "\n",
    "load_streets_and_municipalities_comp = comp.create_component_from_func(load_streets_and_municipalities, base_image=BASE_IMAGE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data_preprocessor import DataPreprocessor \n",
    "\n",
    "# dataPreprocessor = DataPreprocessor(data = real_data_addresses)\n",
    "# processed_data_addresses, metadata_addresses = dataPreprocessor.preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from synthguard.generate_personal_data import PersonalFaker\n",
    "# italian_fake = PersonalFaker(\"it_IT\")\n",
    "\n",
    "# # Generate 10 addresses\n",
    "# synthetic_addresses = italian_fake.generate_data_addresses(street_names, municipality_codes, n_addresses=real_data_addresses.shape[0])\n",
    "\n",
    "# # Print the resulting DataFrame\n",
    "# print(synthetic_addresses.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_addresses(n_rows:int, input_dict: dict, output_csv: comp.OutputPath('csv')):\n",
    "    from synthguard.generate_personal_data import PersonalFaker\n",
    "    from synthguard.helper_functions import save_to_csv\n",
    "    import pandas as pd \n",
    "\n",
    "    italian_fake = PersonalFaker(\"it_IT\")\n",
    "    streets = input_dict['streets']\n",
    "    municipality_codes = [code[0] for code in input_dict['municipalities']]\n",
    "    synthetic_addresses = italian_fake.generate_data_addresses(pd.Series(streets), pd.Series(municipality_codes), n_addresses=n_rows)\n",
    "\n",
    "    print(synthetic_addresses.head())\n",
    "\n",
    "    save_to_csv(synthetic_addresses, output_csv)\n",
    "\n",
    "address_generation_component = comp.create_component_from_func(generate_addresses, base_image=BASE_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# from diagnostic_report_generator import DiagnosticEvaluator\n",
    "\n",
    "# OutputCSV = 'addresses_synthetic.csv'\n",
    "# # output_path = 'synthetic_datasets/teadal/'\n",
    "\n",
    "\n",
    "# if OutputCSV:\n",
    "#     # Create the output path if it does not exist\n",
    "#     import os\n",
    "#     if not os.path.exists(output_path):\n",
    "#         os.makedirs(output_path)\n",
    "#     OutputCSV = output_path + OutputCSV\n",
    "    \n",
    "\n",
    "\n",
    "# diagnosticReportGenerator = DiagnosticEvaluator(real_data = processed_data_addresses, synthetic_data = synthetic_addresses, metadata = metadata_addresses)\n",
    "# diagnosticReportGenerator.run_diagnostic_realistic()\n",
    "# diagnosticReportGenerator.plot_diagnostic_report_realistic(output_path = output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from quality_report_generator import DataQualityEvaluator\n",
    "\n",
    "# dataQualityEvaluator = DataQualityEvaluator(real_data = processed_data_addresses, synthetic_data = synthetic_addresses, metadata = metadata_addresses, method=synthetic_data_type)\n",
    "# dataQualityEvaluator.evaluate_quality()\n",
    "# dataQualityEvaluator.plot_quality_report_realistic(output_path = output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from privacy_report_generator import PrivacyRiskEvaluator\n",
    "\n",
    "# privacyRiskEvaluator = PrivacyRiskEvaluator(real_data = processed_data_addresses, synthetic_data = synthetic_addresses, metadata = metadata_addresses, method=synthetic_data_type)\n",
    "# privacyRiskEvaluator.run_privacy_realistic()\n",
    "# privacyRiskEvaluator.plot_privacy_metrics_realistic(output_path = output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RT-CIT\n",
    "\n",
    "RT-CIT\n",
    "RT CIT thermal group:\n",
    "\n",
    "dimension [cadastre_code] - 10 characters regional code for plant identificationdimension [thermal_unit] - 4 characters thermal group code, ex: GT01\n",
    "\n",
    "\n",
    "dimension [plant_address] - street, number, building, staircase, ...dimension [municipality] - 6 characters istat municipality code, ex: 50001measure [combustion_efficiency] - double precision value between 0 and 1\n",
    "Volume:\n",
    "There are currently 1.851.142 registered plants, 5.595.063 energy efficiency check reports (RCEE) and 3.636 accredited maintenance technicians able to access and update the archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import string\n",
    "# from generate_personal_data import PersonalFaker\n",
    "\n",
    "# # output_path = 'synthetic_datasets/teadal/'\n",
    "\n",
    "# # Create an instance of PersonalFaker\n",
    "# estonian_fake = PersonalFaker(\"et_EE\")\n",
    "\n",
    "# # Lambda functions to generate cadastre and thermal unit codes\n",
    "# generate_cadastre_code = lambda: estonian_fake.generate_code(10)\n",
    "# generate_thermal_unit_code = lambda: \"GT\" + estonian_fake.generate_code(2, string.digits)\n",
    "\n",
    "# # Parameters\n",
    "# n_addresses = synthetic_addresses.shape[0]\n",
    "# n_reports_per_address = 3\n",
    "\n",
    "\n",
    "# cadastre_codes_thermal_units = estonian_fake.generate_cadastre_and_thermal_units(n_addresses, n_reports_per_address)\n",
    "\n",
    "# # generate the rt-cit-thermal-group data\n",
    "# rt_cit_thermal_group = estonian_fake.generate_rt_cit_thermal_group(synthetic_addresses, cadastre_codes_thermal_units, n_reports_per_address)\n",
    "# rt_cit_thermal_group.to_csv(output_path + \"rt-cit-thermal-group.csv\", index=False)\n",
    "\n",
    "\n",
    "# files_to_be_zipped.append(output_path + \"rt-cit-thermal-group.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rtcit(input_csv: comp.InputPath('csv'), output_csv: comp.OutputPath('csv'), n_reports_per_address:int=3):\n",
    "    from synthguard.generate_personal_data import PersonalFaker\n",
    "    from synthguard.helper_functions import load_data_csv\n",
    "\n",
    "    # Create an instance of PersonalFaker\n",
    "    italian_fake = PersonalFaker(\"it_IT\")\n",
    "\n",
    "    synthetic_addresses = load_data_csv(input_csv)\n",
    "\n",
    "    # Parameters\n",
    "    n_addresses = synthetic_addresses.shape[0]\n",
    "\n",
    "    cadastre_codes_thermal_units = italian_fake.generate_cadastre_and_thermal_units(n_addresses, n_reports_per_address)\n",
    "\n",
    "    # generate the rt-cit-thermal-group data\n",
    "    rt_cit_thermal_group = italian_fake.generate_rt_cit_thermal_group(synthetic_addresses, cadastre_codes_thermal_units, n_reports_per_address)\n",
    "    rt_cit_thermal_group.to_csv(output_csv, index=False)\n",
    "\n",
    "rtcit_component = comp.create_component_from_func(rtcit, base_image=BASE_IMAGE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  RT-APE\n",
    "\n",
    "* RT-APE dataset:\n",
    "    * dimension [address]\n",
    "    * dimension [municipality]\n",
    "    * measure [energy_rating]: A4, A3, A2, A1, B, C, D, E, F, G\n",
    "\n",
    "* Volume: \n",
    "    * APE currently contains 420.102 energy performance certificates, 443.142 registered units and 11.214 accredited certifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from generate_personal_data import PersonalFaker\n",
    "# import os\n",
    "\n",
    "# # packages_to_install = [\"xmlschema\"]\n",
    "\n",
    "# # Create an instance of PersonalFaker\n",
    "# RT_APE_fake = PersonalFaker()\n",
    "\n",
    "# # Generate synthetic addresses and municipality codes\n",
    "# addresses = rt_cit_thermal_group['address'].tolist()\n",
    "# municipality_codes = rt_cit_thermal_group['municipality_code'].tolist()\n",
    "\n",
    "# # Define energy ratings and number of certificates\n",
    "# energy_ratings = [\"A4\", \"A3\", \"A2\", \"A1\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"]\n",
    "# num_certificates = 200\n",
    "\n",
    "# # Define file paths for the schema and example XML template\n",
    "# xsd_file_path = os.path.join(input_path, 'rt-ape-schema.xsd')\n",
    "\n",
    "# # Define the path to the example XML file\n",
    "# xml_file_path = os.path.join(input_path, 'rt-ape-example.xml')\n",
    "\n",
    "\n",
    "\n",
    "# # Generate XML files based on the template and schema and return the zipped files path\n",
    "# RT_APE_files = RT_APE_fake.generate_xml_files_from_template(input_path, output_path, xsd_file_path, xml_file_path,\n",
    "#                                          addresses, municipality_codes, num_certificates, energy_ratings,)\n",
    "\n",
    "# # Add the zip file path to the list\n",
    "# files_to_be_zipped.append(RT_APE_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rtape(input_path:str, input_csv: comp.InputPath('csv'), output_zip: comp.OutputPath('zip'), num_certificates:int = 200):\n",
    "    from synthguard.generate_personal_data import PersonalFaker\n",
    "    import os\n",
    "    from synthguard.helper_functions import load_data_csv\n",
    "\n",
    "    # packages_to_install = [\"xmlschema\"]\n",
    "\n",
    "    # Create an instance of PersonalFaker\n",
    "    RT_APE_fake = PersonalFaker()\n",
    "\n",
    "    rt_cit_thermal_group = load_data_csv(input_csv)\n",
    "\n",
    "    # Generate synthetic addresses and municipality codes\n",
    "    addresses = rt_cit_thermal_group['address'].tolist()\n",
    "    municipality_codes = rt_cit_thermal_group['municipality_code'].tolist()\n",
    "\n",
    "    # Define energy ratings and number of certificates\n",
    "    energy_ratings = [\"A4\", \"A3\", \"A2\", \"A1\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"]\n",
    "\n",
    "    # Define file paths for the schema and example XML template\n",
    "    xsd_file_path = os.path.join(input_path, 'rt-ape-schema.xsd')\n",
    "\n",
    "    # Define the path to the example XML file\n",
    "    xml_file_path = os.path.join(input_path, 'rt-ape-example.xml')\n",
    "\n",
    "\n",
    "\n",
    "    # Generate XML files based on the template and schema and return the zipped files path\n",
    "    RT_APE_files = RT_APE_fake.generate_xml_files_from_template(input_path, output_zip, xsd_file_path, xml_file_path,\n",
    "                                            addresses, municipality_codes, num_certificates, energy_ratings,)\n",
    "\n",
    "rtape_component = comp.create_component_from_func(rtape, base_image=BASE_IMAGE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARPAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_arpat(input_path:str, output_pm10_csv: comp.OutputPath('csv'), output_pm25_csv: comp.OutputPath('csv')):\n",
    "    from synthguard.helper_functions import load_json, handle_nested_data_json\n",
    "    import pandas as pd\n",
    "\n",
    "    arpat_file = 'arpat.json'\n",
    "\n",
    "    real_arpat = handle_nested_data_json(pd.json_normalize(load_json(input_path +'/'+ arpat_file)))\n",
    "\n",
    "    pm10_column = 'PM10'\n",
    "    pm2dot5_column = 'PM2dot5'\n",
    "\n",
    "    real_arpat_PM10 = real_arpat.drop(columns=[pm2dot5_column])\n",
    "    real_arpat_PM25 = real_arpat.drop(columns=[pm10_column])\n",
    "\n",
    "    real_arpat_PM10.to_csv(output_pm10_csv)\n",
    "    real_arpat_PM25.to_csv(output_pm25_csv)\n",
    "\n",
    "load_arpat_component = comp.create_component_from_func(load_arpat, base_image=BASE_IMAGE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arpat_preprocessor(input_pm10_csv:comp.InputPath('csv'), \n",
    "                       input_pm25_csv:comp.InputPath('csv'),\n",
    "                       output_pm10_csv: comp.OutputPath('csv'),\n",
    "                       output_pm25_csv: comp.OutputPath('csv'),\n",
    "                       output_pm10_metadata: comp.OutputPath('json'),\n",
    "                       output_pm25_metadata: comp.OutputPath('json')):\n",
    "    from synthguard.data_preprocessor import DataPreprocessor\n",
    "    from synthguard.helper_functions import load_data_csv, save_metadata, save_to_csv\n",
    "\n",
    "    real_arpat_PM10 = load_data_csv(input_pm10_csv)\n",
    "    real_arpat_PM25 = load_data_csv(input_pm25_csv)\n",
    "\n",
    "    pm10_column = 'PM10'\n",
    "    pm2dot5_column = 'PM2dot5'\n",
    "\n",
    "    columns_dict_pm10 = {\n",
    "        \"DATA_OSSERVAZIONE\": \"datetime64[ns]\",\n",
    "        'PM10': 'float64',\n",
    "        'COMUNE': 'string',\n",
    "    }\n",
    "\n",
    "    columns_dict_pm25 = {\n",
    "        \"DATA_OSSERVAZIONE\": \"datetime64[ns]\",\n",
    "        'PM2dot5': 'float64',\n",
    "        'COMUNE': 'string',\n",
    "    }\n",
    "\n",
    "    columns_to_drop = [pm10_column, pm2dot5_column]\n",
    "\n",
    "\n",
    "    dataPreprocessor_arpat_PM10 = DataPreprocessor(data = real_arpat_PM10,)\n",
    "    processed_data_arpat_PM10, metadata_arpat_PM10 = dataPreprocessor_arpat_PM10.preprocess_data(columns_dict=columns_dict_pm10, columns_to_drop=columns_to_drop)\n",
    "\n",
    "    save_metadata(metadata_arpat_PM10, output_pm10_metadata)\n",
    "    save_to_csv(processed_data_arpat_PM10, output_pm10_csv)\n",
    "\n",
    "    dataPreprocessor_arpat_PM25 = DataPreprocessor(data = real_arpat_PM25,)\n",
    "    processed_data_arpat_PM25, metadata_arpat_PM25 = dataPreprocessor_arpat_PM25.preprocess_data(columns_dict=columns_dict_pm25, columns_to_drop=columns_to_drop)\n",
    "\n",
    "    save_metadata(metadata_arpat_PM25, output_pm25_metadata)\n",
    "    save_to_csv(processed_data_arpat_PM25, output_pm25_csv)\n",
    "\n",
    "arpat_preprocessor_component = comp.create_component_from_func(arpat_preprocessor, base_image=BASE_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arpat25_generation(n_rows:int, input_metadata:comp.InputPath('json'), input_preprocess_csv:comp.InputPath('csv'), output_csv:comp.OutputPath('csv')):\n",
    "    import pandas as pd\n",
    "    from synthguard.generate_personal_data import PersonalFaker\n",
    "    from synthguard.synthetic_data_generator import SyntheticDataGenerator\n",
    "    from synthguard.helper_functions import save_to_csv, load_metadata, load_data_csv\n",
    "\n",
    "    N_Rows = n_rows\n",
    "    output_arpat_json_PM25 = 'arpat-synthetic_PM25.csv'\n",
    "\n",
    "    # Create an Italian instance of PersonalFaker\n",
    "    italian_fake = PersonalFaker(\"it_IT\")\n",
    "\n",
    "    it_municipality_codes = italian_fake.generate_administrative_units(N_Rows)\n",
    "    it_municipality_codes\n",
    "\n",
    "    # Generate a list of random dates\n",
    "    random_dates_list = italian_fake.generate_random_dates(N_Rows, \"2023-01-01\", \"2023-12-31\")\n",
    "    random_dates_list\n",
    "\n",
    "    synthetic_arpat_PM25_fake = pd.DataFrame({\n",
    "        'municipality': it_municipality_codes,\n",
    "        'observation_date': random_dates_list\n",
    "    })\n",
    "\n",
    "    processed_data_arpat_PM25 = load_data_csv(input_preprocess_csv)\n",
    "    metadata_arpat_PM25 = load_metadata(input_metadata)\n",
    "\n",
    "\n",
    "    synthetic_arpat_PM25 = SyntheticDataGenerator(n_rows=N_Rows, output_csv=output_arpat_json_PM25, method='realistic').generate_synthetic_data(processed_data_arpat_PM25, metadata_arpat_PM25)\n",
    "    synthetic_arpat_PM25_fake['PM2dot5'] = synthetic_arpat_PM25['PM2dot5']\n",
    "    print(synthetic_arpat_PM25_fake.head())\n",
    "\n",
    "    save_to_csv(synthetic_arpat_PM25_fake, output_csv)\n",
    "\n",
    "arpat25_generation_component = comp.create_component_from_func(arpat25_generation, base_image=BASE_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arpat10_generation(n_rows:int, input_metadata:comp.InputPath('json'), input_preprocess_csv:comp.InputPath('csv'), output_csv:comp.OutputPath('csv')):\n",
    "    import pandas as pd\n",
    "    from synthguard.generate_personal_data import PersonalFaker\n",
    "    from synthguard.helper_functions import save_to_csv, load_metadata, load_data_csv\n",
    "    from synthguard.synthetic_data_generator import SyntheticDataGenerator\n",
    "\n",
    "    output_arpat_json_PM10 = 'arpat-synthetic_PM10.csv'\n",
    "\n",
    "    N_Rows = n_rows\n",
    "\n",
    "    # Create an Italian instance of PersonalFaker\n",
    "    italian_fake = PersonalFaker(\"it_IT\")\n",
    "\n",
    "    it_municipality_codes = italian_fake.generate_administrative_units(N_Rows)\n",
    "    it_municipality_codes\n",
    "\n",
    "    # Generate a list of random dates\n",
    "    random_dates_list = italian_fake.generate_random_dates(N_Rows, \"2023-01-01\", \"2023-12-31\")\n",
    "\n",
    "\n",
    "    synthetic_arpat_PM10_fake = pd.DataFrame({\n",
    "        'municipality': it_municipality_codes,\n",
    "        'observation_date': random_dates_list\n",
    "    })\n",
    "\n",
    "    processed_data_arpat_PM10 = load_data_csv(input_preprocess_csv)\n",
    "    metadata_arpat_PM10 = load_metadata(input_metadata)\n",
    "\n",
    "    synthetic_arpat_PM10 = SyntheticDataGenerator(n_rows=N_Rows, output_csv=output_arpat_json_PM10, method='realistic').generate_synthetic_data(processed_data_arpat_PM10, metadata_arpat_PM10)\n",
    "    synthetic_arpat_PM10_fake['PM10'] = synthetic_arpat_PM10['PM10']\n",
    "    synthetic_arpat_PM10_fake\n",
    "\n",
    "    save_to_csv(synthetic_arpat_PM10_fake, output_csv)\n",
    "\n",
    "arpat10_generation_component = comp.create_component_from_func(arpat10_generation, base_image=BASE_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOX2M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# from generate_personal_data import PersonalFaker\n",
    "\n",
    "# # Usage Example\n",
    "# BOX2M_UNITS = [\n",
    "#     {\"name\": \"Current L1\", \"channel\": 8, \"unit\": \"A\"},\n",
    "#     {\"name\": \"Current L2\", \"channel\": 10, \"unit\": \"A\"},\n",
    "#     {\"name\": \"Current L3\", \"channel\": 12, \"unit\": \"A\"},\n",
    "#     {\"name\": \"Total Active Power\", \"channel\": 58, \"unit\": \"KW\"},\n",
    "#     {\"name\": \"Total Active Energy Import\", \"channel\": 6688, \"unit\": \"KWh\"},\n",
    "# ]\n",
    "\n",
    "# box2m_faker = PersonalFaker(locale=\"it_IT\")\n",
    "# output_box2m_file_path = box2m_faker.generate_box2m_data(\n",
    "#     n_addresses=5, \n",
    "#     output_dir=output_path, \n",
    "#     n_records_per_day=3, \n",
    "#     n_box2m_records=500,\n",
    "#     box2m_units=BOX2M_UNITS\n",
    "# )\n",
    "\n",
    "# files_to_be_zipped.append(output_box2m_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box2m(output_json:comp.OutputPath('json'), n_records:int = 500):\n",
    "    from synthguard.generate_personal_data import PersonalFaker\n",
    "\n",
    "    # Usage Example\n",
    "    BOX2M_UNITS = [\n",
    "        {\"name\": \"Current L1\", \"channel\": 8, \"unit\": \"A\"},\n",
    "        {\"name\": \"Current L2\", \"channel\": 10, \"unit\": \"A\"},\n",
    "        {\"name\": \"Current L3\", \"channel\": 12, \"unit\": \"A\"},\n",
    "        {\"name\": \"Total Active Power\", \"channel\": 58, \"unit\": \"KW\"},\n",
    "        {\"name\": \"Total Active Energy Import\", \"channel\": 6688, \"unit\": \"KWh\"},\n",
    "    ]\n",
    "\n",
    "    box2m_faker = PersonalFaker(locale=\"it_IT\")\n",
    "    output_box2m_file_path = box2m_faker.generate_box2m_data(\n",
    "        n_addresses=5, \n",
    "        output_dir=output_json, \n",
    "        n_records_per_day=3, \n",
    "        n_box2m_records=n_records,\n",
    "        box2m_units=BOX2M_UNITS\n",
    "    )\n",
    "\n",
    "box2m_component = comp.create_component_from_func(box2m, base_image=BASE_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_files(rt_cit: comp.InputPath('csv'), \n",
    "              rt_ape: comp.InputPath('zip'), \n",
    "              sir_temp: comp.InputPath('json'), \n",
    "              arpat25: comp.InputPath('csv'), \n",
    "              arpat10: comp.InputPath('csv'),\n",
    "              box2m: comp.InputPath('json'),\n",
    "              output: comp.OutputPath('zip')):\n",
    "    from synthguard.helper_functions import zip_files\n",
    "    \n",
    "    files_to_be_zipped = [\n",
    "        rt_cit,\n",
    "        rt_ape,\n",
    "        sir_temp,\n",
    "        arpat25,\n",
    "        arpat10,\n",
    "        box2m\n",
    "    ]\n",
    "\n",
    "    zip_files(files_to_be_zipped, output)\n",
    "\n",
    "zip_files_component = comp.create_component_from_func(zip_files, base_image=BASE_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name='test_TEADAL_pipeline')\n",
    "def pipeline(n_rows:int):\n",
    "    #PVC init\n",
    "    existing_pvc = dsl.PipelineVolume(pvc='my-pvc')\n",
    "\n",
    "    input_sir = input_component('/mnt/data/datasets/').add_pvolumes({\"/mnt/data/\": existing_pvc})\n",
    "\n",
    "    preprocess_sir = preprocess_comp(input_sir.outputs['output_min_csv'], input_sir.outputs['output_max_csv'])\n",
    "\n",
    "    generation_sir = generation_comp(n_rows,\n",
    "                                 input_sir.outputs['output_min_csv'], \n",
    "                                 input_sir.outputs['output_max_csv'],\n",
    "                                 preprocess_sir.outputs['output_min_metadata'],\n",
    "                                 preprocess_sir.outputs['output_max_metadata'])\n",
    "    \n",
    "    sir_combine = combine_comp(generation_sir.outputs['output_min_csv'], generation_sir.outputs['output_max_csv'], input_sir.outputs['output_json'])\n",
    "    \n",
    "    diagnostic_sir = diagnostic_component(input_sir.outputs['output_min_csv'], \n",
    "                                 input_sir.outputs['output_max_csv'],\n",
    "                                 generation_sir.outputs['output_min_csv'],\n",
    "                                 generation_sir.outputs['output_max_csv'],\n",
    "                                 preprocess_sir.outputs['output_min_metadata'],\n",
    "                                 preprocess_sir.outputs['output_max_metadata'])\n",
    "    \n",
    "    utility_sir = quality_comp(input_sir.outputs['output_min_csv'], \n",
    "                                 input_sir.outputs['output_max_csv'],\n",
    "                                 generation_sir.outputs['output_min_csv'],\n",
    "                                 generation_sir.outputs['output_max_csv'],\n",
    "                                 preprocess_sir.outputs['output_min_metadata'],\n",
    "                                 preprocess_sir.outputs['output_max_metadata'])\n",
    "    \n",
    "    privacy_sir = privacy_comp(input_sir.outputs['output_min_csv'], \n",
    "                                 input_sir.outputs['output_max_csv'],\n",
    "                                 generation_sir.outputs['output_min_csv'],\n",
    "                                 generation_sir.outputs['output_max_csv'],\n",
    "                                 preprocess_sir.outputs['output_min_metadata'],\n",
    "                                 preprocess_sir.outputs['output_max_metadata'])\n",
    "    \n",
    "    load_streets_and_municipalities = load_streets_and_municipalities_comp('/mnt/data/datasets/').add_pvolumes({\"/mnt/data/\": existing_pvc})\n",
    "\n",
    "    address_generation = address_generation_component(n_rows, load_streets_and_municipalities.output)\n",
    "\n",
    "    rtcit = rtcit_component(address_generation.output)\n",
    "\n",
    "    rtape = rtape_component('/mnt/data/datasets/', rtcit.output).add_pvolumes({\"/mnt/data/\": existing_pvc})\n",
    "    \n",
    "    load_arpat = load_arpat_component('/mnt/data/datasets').add_pvolumes({\"/mnt/data/\": existing_pvc})\n",
    "\n",
    "    arpat_preprocessor = arpat_preprocessor_component(load_arpat.outputs['output_pm10_csv'], load_arpat.outputs['output_pm25_csv'])\n",
    "\n",
    "    arpat25_generation = arpat25_generation_component(n_rows, arpat_preprocessor.outputs['output_pm25_metadata'], arpat_preprocessor.outputs['output_pm25_csv'])\n",
    "\n",
    "    arpat10_generation = arpat10_generation_component(n_rows, arpat_preprocessor.outputs['output_pm10_metadata'], arpat_preprocessor.outputs['output_pm10_csv'])\n",
    "\n",
    "    box2m = box2m_component()\n",
    "\n",
    "    zip_files = zip_files_component(rtcit.output,\n",
    "                          rtape.output,\n",
    "                          sir_combine.outputs['output_combined_json'],\n",
    "                          arpat25_generation.output,\n",
    "                          arpat10_generation.output,\n",
    "                          box2m.output)\n",
    "        \n",
    "Compiler().compile(pipeline, 'TEADAL.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import Client\n",
    "from kubernetes import client as k8s_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_conf = dsl.PipelineConf()\n",
    "pipeline_conf.set_image_pull_secrets([k8s_client.V1ObjectReference(name=\"regcred\")])\n",
    "\n",
    "# Compile the pipeline with the configuration\n",
    "pipeline_path = 'TEADAL_IMAGE.yaml'\n",
    "client = Client()\n",
    "run_result = client.create_run_from_pipeline_func(pipeline, arguments={}, pipeline_conf=pipeline_conf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
